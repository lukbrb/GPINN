@article{WAHEED2021104833,
title = {PINNeik: Eikonal solution using physics-informed neural networks},
journal = {Computers \& Geosciences},
volume = {155},
pages = {104833},
year = {2021},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2021.104833},
url = {https://www.sciencedirect.com/science/article/pii/S009830042100131X},
author = {Umair bin Waheed and Ehsan Haghighat and Tariq Alkhalifah and Chao Song and Qi Hao},
keywords = {Eikonal equation, Physics-informed neural networks, Seismic modeling, Traveltimes},
abstract = {The eikonal equation is utilized across a wide spectrum of science and engineering disciplines. In seismology, it regulates seismic wave traveltimes needed for applications like source localization, imaging, and inversion. Several numerical algorithms have been developed over the years to solve the eikonal equation. However, these methods require considerable modifications to incorporate additional physics, such as anisotropy, and may even breakdown for certain complex forms of the eikonal equation, requiring approximation methods. Moreover, they suffer from computational bottleneck when repeated computations are needed for perturbations in the velocity model and/or the source location, particularly in large 3D models. Here, we propose an algorithm to solve the eikonal equation based on the emerging paradigm of physics-informed neural networks (PINNs). By minimizing a loss function formed by imposing the eikonal equation, we train a neural network to output traveltimes that are consistent with the underlying partial differential equation. We observe sufficiently high traveltime accuracy for most applications of interest. We also demonstrate how the proposed algorithm harnesses machine learning techniques like transfer learning and surrogate modeling to speed up traveltime computations for updated velocity models and source locations. Furthermore, we use a locally adaptive activation function and adaptive weighting of the terms in the loss function to improve convergence rate and solution accuracy. We also show the flexibility of the method in incorporating medium anisotropy and free-surface topography compared to conventional methods that require significant algorithmic modifications. These properties of the proposed PINN eikonal solver are highly desirable in obtaining a flexible and efficient forward modeling engine for seismological applications.}
}
@article{cauchymethode,
  title={M{\'e}thode g{\'e}n{\'e}rale pour la r{\'e}solution des syst{\`e}mes d'{\'e}quations simultan{\'e}es},
  author={Cauchy, Augustin-Louis},
  journal={Comptes rendus hebdomadaires des séances de l'Acad{\'e}mie des sciences},
  volume={25},
  pages={536--538},
  year={1847},
  adsurl={https://gallica.bnf.fr/ark:/12148/bpt6k2982c/f540},
  publisher={Bachelier (Paris)}
}
@article{cybenko1989,
author={Cybenko, G.},
title={Approximation by superpositions of a sigmoidal function},
journal={Mathematics of Control, Signals and Systems},
year={1989},
month={Dec},
day={01},
volume={2},
number={4},
pages={303-314},
abstract={In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
issn={1435-568X},
doi={10.1007/BF02551274},
url={https://doi.org/10.1007/BF02551274}
}

@article{herculano2009human,
  title={The human brain in numbers: a linearly scaled-up primate brain},
  author={Herculano-Houzel, Suzana},
  journal={Frontiers in human neuroscience},
  pages={31},
  year={2009},
  publisher={Frontiers}
}


@article{jumper2021highly,
  title={Highly accurate protein structure prediction with AlphaFold},
  author={Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v{Z}}{\'\i}dek, Augustin and Potapenko, Anna and others},
  journal={Nature},
  volume={596},
  number={7873},
  pages={583--589},
  year={2021},
  publisher={Nature Publishing Group UK London}
}


@article{eglajs1977new,
  title={New approach to the design of multifactor experiments},
  author={Eglajs, V and Audze, P},
  journal={Problems of Dynamics and Strengths},
  volume={35},
  number={1},
  pages={104--107},
  year={1977},
  publisher={Zinatne Publishing House Riga, Latvia}
}

@article{mckay1979comparison,
  title={A Comparison of Three Methods for Selecting Values of Input Variables in the Analysis of Output from a Computer Code, 1979},
  author={McKay, MD and Beckham, RJ and Conover, WJ},
  journal={Technometrics},
  volume={21},
  pages={21},
  year={1979}
}


@article{hornik1991approximation,
  title={Approximation capabilities of multilayer feedforward networks},
  author={Hornik, Kurt},
  journal={Neural networks},
  volume={4},
  number={2},
  pages={251--257},
  year={1991},
  publisher={Elsevier}
}

@article{nocedal1980updating,
  title={Updating quasi-Newton matrices with limited storage},
  author={Nocedal, Jorge},
  journal={Mathematics of computation},
  volume={35},
  number={151},
  pages={773--782},
  year={1980}
}


@misc{kingma2017adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{raissi2017physics2,
      title={Physics Informed Deep Learning (Part II): Data-driven Discovery of Nonlinear Partial Differential Equations}, 
      author={Maziar Raissi and Paris Perdikaris and George Em Karniadakis},
      year={2017},
      eprint={1711.10566},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
@article{rosenblatt1958perceptron,
  title={The perceptron: a probabilistic model for information storage and organization in the brain.},
  author={Rosenblatt, Frank},
  journal={Psychological review},
  volume={65},
  number={6},
  pages={386},
  year={1958},
  publisher={American Psychological Association}
}
@article{rumelhart1986learning,
  title={Learning representations by back-propagating errors},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  journal={nature},
  volume={323},
  number={6088},
  pages={533--536},
  year={1986},
  publisher={Nature Publishing Group UK London}
}
@article{schmidhuber2015deep,
  title={Deep learning in neural networks: An overview},
  author={Schmidhuber, J{\"u}rgen},
  journal={Neural networks},
  volume={61},
  pages={85--117},
  year={2015},
  publisher={Elsevier}
}
@incollection{lecun2002efficient,
  title={Efficient backprop},
  author={LeCun, Yann and Bottou, L{\'e}on and Orr, Genevieve B and M{\"u}ller, Klaus-Robert},
  booktitle={Neural networks: Tricks of the trade},
  pages={9--50},
  year={2002},
  publisher={Springer}
}
@article{lecun1995convolutional,
  title={Convolutional networks for images, speech, and time series},
  author={LeCun, Yann and Bengio, Yoshua and others},
  journal={The handbook of brain theory and neural networks},
  volume={3361},
  number={10},
  pages={1995},
  year={1995},
  publisher={Citeseer}
}

@incollection{NEURIPS2019_9015,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@ARTICLE{2020SciPy-NMeth,
  author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
            Haberland, Matt and Reddy, Tyler and Cournapeau, David and
            Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
            Bright, Jonathan and {van der Walt}, St{\'e}fan J. and
            Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
            Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
            Kern, Robert and Larson, Eric and Carey, C J and
            Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and
            {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
            Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
            Harris, Charles R. and Archibald, Anne M. and
            Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and
            {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific
            Computing in Python}},
  journal = {Nature Methods},
  year    = {2020},
  volume  = {17},
  pages   = {261--272},
  adsurl  = {https://rdcu.be/b08Wh},
  doi     = {10.1038/s41592-019-0686-2},
}

@article{HE2020103610,
title = {Physics-informed neural networks for multiphysics data assimilation with application to subsurface transport},
journal = {Advances in Water Resources},
volume = {141},
pages = {103610},
year = {2020},
issn = {0309-1708},
doi = {https://doi.org/10.1016/j.advwatres.2020.103610},
url = {https://www.sciencedirect.com/science/article/pii/S0309170819311649},
author = {QiZhi He and David Barajas-Solano and Guzel Tartakovsky and Alexandre M. Tartakovsky},
keywords = {Physics-informed deep neural networks, Data assimilation, Parameter estimation, Inverse problems, Subsurface flow and transport},
abstract = {Data assimilation for parameter and state estimation in subsurface transport problems remains a significant challenge because of the sparsity of measurements, the heterogeneity of porous media, and the high computational cost of forward numerical models. We present a multiphysics-informed deep neural network machine learning method for estimating space-dependent hydraulic conductivity, hydraulic head, and concentration fields from sparse measurements. In this approach, we employ individual deep neural networks (DNNs) to approximate the unknown parameters (e.g., hydraulic conductivity) and states (e.g., hydraulic head and concentration) of a physical system. Next, we jointly train these DNNs by minimizing the loss function that consists of the governing equations residuals in addition to the error with respect to measurement data. We apply this approach to assimilate conductivity, hydraulic head, and concentration measurements for the joint inversion of these parameter and states in a steady-state advection–dispersion problem. We study the accuracy of the proposed data assimilation approach with respect to the data size (i.e., the number of measured variables and the number of measurements of each variable), DNN size, and the complexity of the parameter field. We demonstrate that the physics-informed DNNs are significantly more accurate than the standard data-driven DNNs, especially when the training set consists of sparse data. We also show that the accuracy of parameter estimation increases as more different multiphysics variables are inverted jointly.}
}

@inproceedings{deVaucouleurs1948,
  title={Recherches sur les nebuleuses extragalactiques},
  author={de Vaucouleurs, Gerard},
  booktitle={Annales d'Astrophysique},
  volume={11},
  pages={247},
  year={1948}
}

@article{jaffe1983simple,
  title={A simple model for the distribution of light in spherical galaxies},
  author={Jaffe, Walter},
  journal={Monthly Notices of the Royal Astronomical Society},
  volume={202},
  number={4},
  pages={995--999},
  year={1983},
  publisher={The Royal Astronomical Society}
}

@book{binney2011galactic,
  title={Galactic dynamics},
  author={Binney, James and Tremaine, Scott},
  volume={20},
  year={2011},
  publisher={Princeton university press}
}

@article{rigamonti2022maximally,
  title={Maximally informed Bayesian modelling of disc galaxies},
  author={Rigamonti, Fabio and Dotti, Massimo and Covino, Stefano and Haardt, Francesco and Landoni, Marco and Del Pozzo, Walter and Lupi, Alessandro and Zibetti, Stefano},
  journal={Monthly Notices of the Royal Astronomical Society},
  volume={513},
  number={4},
  pages={6111--6124},
  year={2022},
  publisher={Oxford University Press}
}

@article{freeman1970disks,
  title={On the disks of spiral and S0 galaxies},
  author={Freeman, Kenneth C},
  journal={Astrophysical Journal, vol. 160, p. 811},
  volume={160},
  pages={811},
  year={1970}
}
@article{dissanayake1994,
author = {Dissanayake, M. W. M. G. and Phan-Thien, N.},
title = {Neural-network-based approximations for solving partial differential equations},
journal = {Communications in Numerical Methods in Engineering},
volume = {10},
number = {3},
pages = {195-201},
doi = {https://doi.org/10.1002/cnm.1640100303},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cnm.1640100303},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cnm.1640100303},
abstract = {Abstract A numerical method, based on neural-network-based functions, for solving partial differential equations is reported in the paper. Using a ‘universal approximator’ based on a neural network and point collocation, the numerical problem of solving the partial differential equation is transformed to an unconstrained minimization problem. The method is extremely easy to implement and is suitable for obtaining an approximate solution in a short period of time. The technique is illustrated with the aid of two numerical examples.},
year = {1994}
}

@article{smith2015simple,
  title={Simple and accurate modelling of the gravitational potential produced by thick and thin exponential discs},
  author={Smith, Rory and Flynn, Chris and Candlish, Graeme N and Fellhauer, Michael and Gibson, Bradley Kenneth},
  journal={Monthly Notices of the Royal Astronomical Society},
  volume={448},
  number={3},
  pages={2934--2940},
  year={2015},
  publisher={Oxford University Press}
}
@article{bonetti2021dynamical,
  title={Dynamical evolution of massive perturbers in realistic multicomponent galaxy models I: implementation and validation},
  author={Bonetti, Matteo and Bortolas, Elisa and Lupi, Alessandro and Dotti, Massimo},
  journal={Monthly Notices of the Royal Astronomical Society},
  volume={502},
  number={3},
  pages={3554--3568},
  year={2021},
  publisher={Oxford University Press}
}
@article{miyamoto1975three,
  title={Three-dimensional models for the distribution of mass in galaxies},
  author={Miyamoto, Masanori and Nagai, Ryuzaburo},
  journal={Astronomical Society of Japan, Publications, vol. 27, no. 4, 1975, p. 533-543.},
  volume={27},
  pages={533--543},
  year={1975}
}

@article{raissi_physics-informed_2019,
	title = {Physics-informed neural networks: {A} deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
	volume = {378},
	issn = {00219991},
	shorttitle = {Physics-informed neural networks},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0021999118307125},
	doi = {10.1016/j.jcp.2018.10.045},
	language = {en},
	urldate = {2023-03-19},
	journal = {Journal of Computational Physics},
	author = {Raissi, M. and Perdikaris, P. and Karniadakis, G.E.},
	month = feb,
	year = {2019},
	pages = {686--707},
	file = {Raissi et al. - 2019 - Physics-informed neural networks A deep learning .pdf:/Users/lucas/Zotero/storage/V5QLAY4B/Raissi et al. - 2019 - Physics-informed neural networks A deep learning .pdf:application/pdf},
}

@article{pang_fpinns_2019,
	title = {{fPINNs}: {Fractional} {Physics}-{Informed} {Neural} {Networks}},
	volume = {41},
	issn = {1064-8275, 1095-7197},
	shorttitle = {{fPINNs}},
	url = {http://arxiv.org/abs/1811.08967},
	doi = {10.1137/18M1229845},
	abstract = {Physics-informed neural networks (PINNs), introduced in [1], are eﬀective in solving integerorder partial diﬀerential equations (PDEs) based on scattered and noisy data. PINNs employ standard feedforward neural networks (NNs) with the PDEs explicitly encoded into the NN using automatic diﬀerentiation, while the sum of the mean-squared PDE-residuals and the mean-squared error in initial/boundary conditions is minimized with respect to the NN parameters. Here we extend PINNs to fractional PINNs (fPINNs) to solve space-time fractional advection-diﬀusion equations (fractional ADEs), and we study systematically their convergence, hence explaining both of fPINNs and PINNs for ﬁrst time. Speciﬁcally, we demonstrate their accuracy and eﬀectiveness in solving multi-dimensional forward and inverse problems with forcing terms whose values are only known at randomly scattered spatio-temporal coordinates (black-box forcing terms). A novel element of the fPINNs is the hybrid approach that we introduce for constructing the residual in the loss function using both automatic diﬀerentiation for the integer-order operators and numerical discretization for the fractional operators. This approach bypasses the diﬃculties stemming from the fact that automatic differentiation is not applicable to fractional operators because the standard chain rule in integer calculus is not valid in fractional calculus. To discretize the fractional operators, we employ the Grünwald-Letnikov (GL) formula in one-dimensional fractional ADEs and the vector GL formula in conjunction with the directional fractional Laplacian in two- and three-dimensional fractional ADEs. We ﬁrst consider the one-dimensional fractional Poisson equation and compare the convergence of the fPINNs against the ﬁnite diﬀerence method (FDM). We present the solution convergence using both the mean L2 error as well as the standard deviation due to sensitivity to NN parameter initializations. Using diﬀerent GL formulas we observe ﬁrst-, second-, and third-order convergence rates for small size of training sets but the error saturates for larger training sets. We explain these results by analyzing the four sources of numerical errors due to discretization, sampling, NN approximation, and optimization. The total error decays monotonically (below 10−5 for third order GL formula) but it saturates beyond that point due to the optimization error. We also analyze the relative balance between discretization and sampling errors and observe that the sampling size and the number of discretization points (auxiliary points) should be comparable to achieve the highest accuracy. As we increase the depth of the NN up to certain value, the mean error decreases and the standard deviation increases whereas the width has essentially no eﬀect unless its value is either too small or too large. We next consider time-dependent fractional ADEs and compare white-box (WB) and black-box (BB) forcing. We observe that for the WB forcing, our results are similar to the aforementioned cases, however, for the BB forcing fPINNs outperform FDM. Subsequently, we consider multi-dimensional time-, space-, and space-time-fractional ADEs using the directional fractional Laplacian and we observe relative errors of 10−3 ∼ 10−4. Finally, we solve several inverse problems in 1D, 2D, and 3D to identify the fractional orders, diﬀusion coeﬃcients, and transport velocities and obtain accurate results given proper initializations even in the presence of signiﬁcant noise.},
	language = {en},
	number = {4},
	urldate = {2023-03-19},
	journal = {SIAM Journal on Scientific Computing},
	author = {Pang, Guofei and Lu, Lu and Karniadakis, George Em},
	month = jan,
	year = {2019},
	note = {arXiv:1811.08967 [physics]},
	keywords = {Physics - Computational Physics},
	pages = {A2603--A2626},
	annote = {Comment: 29 pages, 15 figures, 5 tables},
	annote = {Comment: 29 pages, 15 figures, 5 tables},
	file = {Pang et al. - 2019 - fPINNs Fractional Physics-Informed Neural Network.pdf:/Users/lucas/Zotero/storage/PV735RZC/Pang et al. - 2019 - fPINNs Fractional Physics-Informed Neural Network.pdf:application/pdf},
}

@article{mao_physics-informed_2020,
	title = {Physics-informed neural networks for high-speed flows},
	volume = {360},
	issn = {00457825},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0045782519306814},
	doi = {10.1016/j.cma.2019.112789},
	abstract = {In this work we investigate the possibility of using physics-informed neural networks (PINNs) to approximate the Euler equations that model high-speed aerodynamic flows. In particular, we solve both the forward and inverse problems in onedimensional and two-dimensional domains. For the forward problem, we utilize the Euler equations and the initial/boundary conditions to formulate the loss function, and solve the one-dimensional Euler equations with smooth solutions and with solutions that have a contact discontinuity as well as a two-dimensional oblique shock wave problem. We demonstrate that we can capture the solutions with only a few scattered points clustered randomly around the discontinuities. For the inverse problem, motivated by mimicking the Schlieren photography experimental technique used traditionally in high-speed aerodynamics, we use the data on density gradient ∇ρ(x, t), the pressure p(x∗, t) at a specified point x = x∗ as well as the conservation laws to infer all states of interest (density, velocity and pressure fields). We present illustrative benchmark examples for both the problem with smooth solutions and Riemann problems (Sod and Lax problems) with PINNs, demonstrating that all inferred states are in good agreement with the reference solutions. Moreover, we show that the choice of the position of the point x∗ plays an important role in the learning process. In particular, for the problem with smooth solutions we can randomly choose the position of the point x∗ from the computational domain, while for the Sod or Lax problem, we have to choose the position of the point x∗ from the domain between the initial discontinuous point and the shock position of the final time. We also solve the inverse problem by combining the aforementioned data and the Euler equations in characteristic form, showing that the results obtained by using the Euler equations in characteristic form are better than that obtained by using the Euler equations in conservative form. Furthermore, we consider another type of inverse problem, specifically, we employ PINNs to learn the value of the parameter γ in the equation of state for the parameterized two-dimensional oblique wave problem by using the given data of the density, velocity and the pressure, and we identify the parameter γ accurately. Taken together, our results demonstrate that in the current form, where the conservation laws are imposed at random points, PINNs are not as accurate as traditional numerical methods for forward problems but they are superior for inverse problems that cannot even be solved with standard techniques.},
	language = {en},
	urldate = {2023-03-19},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Mao, Zhiping and Jagtap, Ameya D. and Karniadakis, George Em},
	month = mar,
	year = {2020},
	pages = {112789},
	file = {Mao et al. - 2020 - Physics-informed neural networks for high-speed fl.pdf:/Users/lucas/Zotero/storage/SN2I4UHD/Mao et al. - 2020 - Physics-informed neural networks for high-speed fl.pdf:application/pdf},
}

@misc{kharazmi_variational_2019,
	title = {Variational {Physics}-{Informed} {Neural} {Networks} {For} {Solving} {Partial} {Differential} {Equations}},
	url = {http://arxiv.org/abs/1912.00873},
	abstract = {Physics-informed neural networks (PINNs) [31] use automatic diﬀerentiation to solve partial diﬀerential equations (PDEs) by penalizing the PDE in the loss function at a random set of points in the domain of interest. Here, we develop a Petrov-Galerkin version of PINNs based on the nonlinear approximation of deep neural networks (DNNs) by selecting the trial space to be the space of neural networks and the test space to be the space of Legendre polynomials. We formulate the variational residual of the PDE using the DNN approximation by incorporating the variational form of the problem into the loss function of the network and construct a variational physics-informed neural network (VPINN). By integrating by parts the integrand in the variational form, we lower the order of the differential operators represented by the neural networks, hence eﬀectively reducing the training cost in VPINNs while increasing their accuracy compared to PINNs that essentially employ delta test functions. For shallow networks with one hidden layer, we analytically obtain explicit forms of the variational residual. We demonstrate the performance of the new formulation for several examples that show clear advantages of VPINNs over PINNs in terms of both accuracy and speed.},
	language = {en},
	urldate = {2023-03-19},
	publisher = {arXiv},
	author = {Kharazmi, E. and Zhang, Z. and Karniadakis, G. E.},
	month = nov,
	year = {2019},
	note = {arXiv:1912.00873 [physics, stat]},
	keywords = {Physics - Computational Physics, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Numerical Analysis, Statistics - Machine Learning},
	annote = {Comment: 24 pages, 12 figures},
	annote = {Comment: 24 pages, 12 figures},
	annote = {Comment: 24 pages, 12 figures},
	file = {Kharazmi et al. - 2019 - Variational Physics-Informed Neural Networks For S.pdf:/Users/lucas/Zotero/storage/TG3R3FY3/Kharazmi et al. - 2019 - Variational Physics-Informed Neural Networks For S.pdf:application/pdf;Kharazmi et al. - 2019 - Variational Physics-Informed Neural Networks For S.pdf:/Users/lucas/Zotero/storage/VRBFBMJC/Kharazmi et al. - 2019 - Variational Physics-Informed Neural Networks For S.pdf:application/pdf},
}

@misc{misyris_physics-informed_2020,
	title = {Physics-{Informed} {Neural} {Networks} for {Power} {Systems}},
	url = {http://arxiv.org/abs/1911.03737},
	abstract = {This paper introduces for the ﬁrst time, to our knowledge, a framework for physics-informed neural networks in power system applications. Exploiting the underlying physical laws governing power systems, and inspired by recent developments in the ﬁeld of machine learning, this paper proposes a neural network training procedure that can make use of the wide range of mathematical models describing power system behavior, both in steady-state and in dynamics. Physics-informed neural networks require substantially less training data and can result in simpler neural network structures, while achieving high accuracy. This work unlocks a range of opportunities in power systems, being able to determine dynamic states, such as rotor angles and frequency, and uncertain parameters such as inertia and damping at a fraction of the computational time required by conventional methods. This paper focuses on introducing the framework and showcases its potential using a single-machine inﬁnite bus system as a guiding example. Physics-informed neural networks are shown to accurately determine rotor angle and frequency up to 87 times faster than conventional methods.},
	language = {en},
	urldate = {2023-03-19},
	publisher = {arXiv},
	author = {Misyris, George S. and Venzke, Andreas and Chatzivasileiadis, Spyros},
	month = jan,
	year = {2020},
	note = {arXiv:1911.03737 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing, Electrical Engineering and Systems Science - Systems and Control},
	file = {Misyris et al. - 2020 - Physics-Informed Neural Networks for Power Systems.pdf:/Users/lucas/Zotero/storage/NDGZBQUQ/Misyris et al. - 2020 - Physics-Informed Neural Networks for Power Systems.pdf:application/pdf},
}

@article{krishnapriyan_characterizing_nodate,
	title = {Characterizing possible failure modes in physics-informed neural networks},
	abstract = {Recent work in scientiﬁc machine learning has developed so-called physicsinformed neural network (PINN) models. The typical approach is to incorporate physical domain knowledge as soft constraints on an empirical loss function and use existing machine learning methodologies to train the model. We demonstrate that, while existing PINN methodologies can learn good models for relatively trivial problems, they can easily fail to learn relevant physical phenomena for even slightly more complex problems. In particular, we analyze several distinct situations of widespread physical interest, including learning differential equations with convection, reaction, and diffusion operators. We provide evidence that the soft regularization in PINNs, which involves PDE-based differential operators, can introduce a number of subtle problems, including making the problem more ill-conditioned. Importantly, we show that these possible failure modes are not due to the lack of expressivity in the NN architecture, but that the PINN’s setup makes the loss landscape very hard to optimize. We then describe two promising solutions to address these failure modes. The ﬁrst approach is to use curriculum regularization, where the PINN’s loss term starts from a simple PDE regularization, and becomes progressively more complex as the NN gets trained. The second approach is to pose the problem as a sequence-to-sequence learning task, rather than learning to predict the entire space-time at once. Extensive testing shows that we can achieve up to 1-2 orders of magnitude lower error with these methods as compared to regular PINN training.},
	language = {en},
	author = {Krishnapriyan, Aditi S and Gholami, Amir and Zhe, Shandian and Kirby, Robert M and Mahoney, Michael W},
	file = {Krishnapriyan et al. - Characterizing possible failure modes in physics-i.pdf:/Users/lucas/Zotero/storage/X59LB2NL/Krishnapriyan et al. - Characterizing possible failure modes in physics-i.pdf:application/pdf},
}

@article{cai_physics-informed_2021,
	title = {Physics-informed neural networks ({PINNs}) for fluid mechanics: a review},
	volume = {37},
	issn = {0567-7718, 1614-3116},
	shorttitle = {Physics-informed neural networks ({PINNs}) for fluid mechanics},
	url = {https://link.springer.com/10.1007/s10409-021-01148-1},
	doi = {10.1007/s10409-021-01148-1},
	abstract = {Despite the signiﬁcant progress over the last 50 years in simulating ﬂow problems using numerical discretization of the Navier–Stokes equations (NSE), we still cannot incorporate seamlessly noisy data into existing algorithms, mesh-generation is complex, and we cannot tackle high-dimensional problems governed by parametrized NSE. Moreover, solving inverse ﬂow problems is often prohibitively expensive and requires complex and expensive formulations and new computer codes. Here, we review ﬂow physics-informed learning, integrating seamlessly data and mathematical models, and implement them using physics-informed neural networks (PINNs). We demonstrate the effectiveness of PINNs for inverse problems related to three-dimensional wake ﬂows, supersonic ﬂows, and biomedical ﬂows.},
	language = {en},
	number = {12},
	urldate = {2023-03-19},
	journal = {Acta Mechanica Sinica},
	author = {Cai, Shengze and Mao, Zhiping and Wang, Zhicheng and Yin, Minglang and Karniadakis, George Em},
	month = dec,
	year = {2021},
	pages = {1727--1738},
	file = {Cai et al. - 2021 - Physics-informed neural networks (PINNs) for fluid.pdf:/Users/lucas/Zotero/storage/ENLAVZXJ/Cai et al. - 2021 - Physics-informed neural networks (PINNs) for fluid.pdf:application/pdf},
}

@article{cai_physics-informed_2021-1,
	title = {Physics-{Informed} {Neural} {Networks} for {Heat} {Transfer} {Problems}},
	volume = {143},
	issn = {0022-1481, 1528-8943},
	url = {https://asmedigitalcollection.asme.org/heattransfer/article/143/6/060801/1104439/Physics-Informed-Neural-Networks-for-Heat-Transfer},
	doi = {10.1115/1.4050542},
	abstract = {Abstract
            Physics-informed neural networks (PINNs) have gained popularity across different engineering fields due to their effectiveness in solving realistic problems with noisy data and often partially missing physics. In PINNs, automatic differentiation is leveraged to evaluate differential operators without discretization errors, and a multitask learning problem is defined in order to simultaneously fit observed data while respecting the underlying governing laws of physics. Here, we present applications of PINNs to various prototype heat transfer problems, targeting in particular realistic conditions not readily tackled with traditional computational methods. To this end, we first consider forced and mixed convection with unknown thermal boundary conditions on the heated surfaces and aim to obtain the temperature and velocity fields everywhere in the domain, including the boundaries, given some sparse temperature measurements. We also consider the prototype Stefan problem for two-phase flow, aiming to infer the moving interface, the velocity and temperature fields everywhere as well as the different conductivities of a solid and a liquid phase, given a few temperature measurements inside the domain. Finally, we present some realistic industrial applications related to power electronics to highlight the practicality of PINNs as well as the effective use of neural networks in solving general heat transfer problems of industrial complexity. Taken together, the results presented herein demonstrate that PINNs not only can solve ill-posed problems, which are beyond the reach of traditional computational methods, but they can also bridge the gap between computational and experimental heat transfer.},
	language = {en},
	number = {6},
	urldate = {2023-03-19},
	journal = {Journal of Heat Transfer},
	author = {Cai, Shengze and Wang, Zhicheng and Wang, Sifan and Perdikaris, Paris and Karniadakis, George Em},
	month = jun,
	year = {2021},
	pages = {060801},
	file = {Cai et al. - 2021 - Physics-Informed Neural Networks for Heat Transfer.pdf:/Users/lucas/Zotero/storage/W755G2AH/Cai et al. - 2021 - Physics-Informed Neural Networks for Heat Transfer.pdf:application/pdf},
}

@article{wang_when_2022,
	title = {When and why {PINNs} fail to train: {A} neural tangent kernel perspective},
	volume = {449},
	issn = {00219991},
	shorttitle = {When and why {PINNs} fail to train},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S002199912100663X},
	doi = {10.1016/j.jcp.2021.110768},
	language = {en},
	urldate = {2023-03-19},
	journal = {Journal of Computational Physics},
	author = {Wang, Sifan and Yu, Xinling and Perdikaris, Paris},
	month = jan,
	year = {2022},
	pages = {110768},
	file = {Wang et al. - 2022 - When and why PINNs fail to train A neural tangent.pdf:/Users/lucas/Zotero/storage/5FGLXYAW/Wang et al. - 2022 - When and why PINNs fail to train A neural tangent.pdf:application/pdf},
}

@article{cuomo_scientific_2022,
	title = {Scientific {Machine} {Learning} {Through} {Physics}–{Informed} {Neural} {Networks}: {Where} we are and {What}’s {Next}},
	volume = {92},
	issn = {0885-7474, 1573-7691},
	shorttitle = {Scientific {Machine} {Learning} {Through} {Physics}–{Informed} {Neural} {Networks}},
	url = {https://link.springer.com/10.1007/s10915-022-01939-z},
	doi = {10.1007/s10915-022-01939-z},
	abstract = {Physics-Informed Neural Networks (PINN) are neural networks (NNs) that encode model equations, like Partial Differential Equations (PDE), as a component of the neural network itself. PINNs are nowadays used to solve PDEs, fractional equations, integral-differential equations, and stochastic PDEs. This novel methodology has arisen as a multi-task learning framework in which a NN must ﬁt observed data while reducing a PDE residual. This article provides a comprehensive review of the literature on PINNs: while the primary goal of the study was to characterize these networks and their related advantages and disadvantages. The review also attempts to incorporate publications on a broader range of collocation-based physics informed neural networks, which stars form the vanilla PINN, as well as many other variants, such as physics-constrained neural networks (PCNN), variational hp-VPINN, and conservative PINN (CPINN). The study indicates that most research has focused on customizing the PINN through different activation functions, gradient optimization techniques, neural network structures, and loss function structures. Despite the wide range of applications for which PINNs have been used, by demonstrating their ability to be more feasible in some contexts than classical numerical techniques like Finite Element Method (FEM), advancements are still possible, most notably theoretical issues that remain unresolved.},
	language = {en},
	number = {3},
	urldate = {2023-03-19},
	journal = {Journal of Scientific Computing},
	author = {Cuomo, Salvatore and Di Cola, Vincenzo Schiano and Giampaolo, Fabio and Rozza, Gianluigi and Raissi, Maziar and Piccialli, Francesco},
	month = sep,
	year = {2022},
	pages = {88},
	file = {Cuomo et al. - 2022 - Scientific Machine Learning Through Physics–Inform.pdf:/Users/lucas/Zotero/storage/FQR6IDIN/Cuomo et al. - 2022 - Scientific Machine Learning Through Physics–Inform.pdf:application/pdf;Cuomo et al. - 2022 - Scientific Machine Learning Through Physics–Inform.pdf:/Users/lucas/Zotero/storage/YMSZLBY6/Cuomo et al. - 2022 - Scientific Machine Learning Through Physics–Inform.pdf:application/pdf},
}

@article{hernquist_analytical_1990,
	title = {An analytical model for spherical galaxies and bulges},
	volume = {356},
	issn = {0004-637X, 1538-4357},
	url = {http://adsabs.harvard.edu/doi/10.1086/168845},
	doi = {10.1086/168845},
	abstract = {A potential-density pair which closely approximates the de Vaucouleurs R1/4 law for elliptical galaxies is presented. It is shown that the intrinsic properties and projected distributions of this model can be evaluated analytically. In particular, the distribution function, density of states, and projected surface brightness and velocity dispersion are expressible in terms of elementary functions.},
	language = {en},
	urldate = {2023-03-19},
	journal = {The Astrophysical Journal},
	author = {Hernquist, Lars},
	month = jun,
	year = {1990},
	pages = {359},
	file = {Hernquist - 1990 - An analytical model for spherical galaxies and bul.pdf:/Users/lucas/Zotero/storage/JPNQDPHY/Hernquist - 1990 - An analytical model for spherical galaxies and bul.pdf:application/pdf},
}

@article{dehnen_family_1993,
	title = {A family of potential-density pairs for spherical galaxies and bulges},
	volume = {265},
	issn = {0035-8711, 1365-2966},
	url = {https://academic.oup.com/mnras/article-lookup/doi/10.1093/mnras/265.1.250},
	doi = {10.1093/mnras/265.1.250},
	abstract = {A family of spherical potential-density pairs is presented. The densities are proportional to r{\textasciitilde}4 at large radii and diverge in the centre as r{\textasciitilde}y with 0{\textless}y{\textless}3. The models of Jaffe and Hernquist are included as special cases. The gravitational potential is analytical for all y. For specific values of y, most of the intrinsic and projected properties, such as distribution function, surface density and projected velocity dispersion, can be expressed in terms of elementary functions. A comparison to the de Vaucouleurs R1//4-profile shows that the model with y = 3/2 most closely resembles it in both surface density and distribution function. This particular model is completely analytical, and thus it is the best analytical approximation of the R1/4-model known so far.},
	language = {en},
	number = {1},
	urldate = {2023-03-19},
	journal = {Monthly Notices of the Royal Astronomical Society},
	author = {Dehnen, W.},
	month = nov,
	year = {1993},
	pages = {250--256},
	file = {Dehnen - 1993 - A family of potential-density pairs for spherical .pdf:/Users/lucas/Zotero/storage/6B5X2LST/Dehnen - 1993 - A family of potential-density pairs for spherical .pdf:application/pdf},
}

@article{barber_following_nodate,
	title = {Following the mathematics of {Hernquist} 1990},
	language = {en},
	author = {Barber, J},
	file = {Barber - Following the mathematics of Hernquist 1990.pdf:/Users/lucas/Zotero/storage/BDI88I6P/Barber - Following the mathematics of Hernquist 1990.pdf:application/pdf},
}

@article{caravita_jeans_2021,
	title = {Jeans modeling of axisymmetric galaxies with multiple stellar populations},
	volume = {506},
	issn = {0035-8711, 1365-2966},
	url = {http://arxiv.org/abs/2102.09440},
	doi = {10.1093/mnras/stab1786},
	abstract = {We present the theoretical framework to eﬃciently solve the Jeans equations for multi-component axisymmetric stellar systems, focusing on the scaling of all quantities entering them. The models may include an arbitrary number of stellar distributions, a dark matter halo, and a central supermassive black hole; each stellar distribution is implicitly described by a two- or three-integral distribution function, and the stellar components can have diﬀerent structural (density proﬁle, ﬂattening, mass, scale-length), dynamical (rotation, velocity dispersion anisotropy), and population (age, metallicity, initial mass function, mass-to-light ratio) properties. In order to determine the ordered rotational velocity and the azimuthal velocity dispersion ﬁelds of each component, we introduce a decomposition that can be used when the commonly adopted Satoh decomposition cannot be applied. The scheme developed is particularly suitable for a numerical implementation; we describe its realisation within our code JASMINE2, optimised to maximally exploit the scalings allowed by the Poisson and the Jeans equations, also in the post-processing procedures. As applications, we illustrate the building of three multi-component galaxy models with two distinct stellar populations, a central black hole, and a dark matter halo; we also study the solution of the Jeans equations for an exponential thick disc, and for its multi-component representation as the superposition of three Miyamoto-Nagai discs. A useful general formula for the numerical evaluation of the gravitational potential of factorised thick discs is ﬁnally given.},
	language = {en},
	number = {1},
	urldate = {2023-03-19},
	journal = {Monthly Notices of the Royal Astronomical Society},
	author = {Caravita, C. and Ciotti, L. and Pellegrini, S.},
	month = jul,
	year = {2021},
	note = {arXiv:2102.09440 [astro-ph]},
	keywords = {Astrophysics - Astrophysics of Galaxies},
	pages = {1480--1497},
	annote = {Comment: 18 pages, 7 figures, MNRAS accepted. Significant changes in response to the referee's comments; a new section is devoted to the application of the code to exponential thick discs},
	annote = {Comment: 18 pages, 7 figures, MNRAS accepted. Significant changes in response to the referee's comments; a new section is devoted to the application of the code to exponential thick discs},
	file = {Caravita et al. - 2021 - Jeans modeling of axisymmetric galaxies with multi.pdf:/Users/lucas/Zotero/storage/2PMYTLRM/Caravita et al. - 2021 - Jeans modeling of axisymmetric galaxies with multi.pdf:application/pdf},
}

@misc{raissi_physics_2017,
	title = {Physics {Informed} {Deep} {Learning} ({Part} {I}): {Data}-driven {Solutions} of {Nonlinear} {Partial} {Differential} {Equations}},
	shorttitle = {Physics {Informed} {Deep} {Learning} ({Part} {I})},
	url = {http://arxiv.org/abs/1711.10561},
	abstract = {We introduce physics informed neural networks – neural networks that are trained to solve supervised learning tasks while respecting any given law of physics described by general nonlinear partial diﬀerential equations. In this two part treatise, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial diﬀerential equations. Depending on the nature and arrangement of the available data, we devise two distinct classes of algorithms, namely continuous time and discrete time models. The resulting neural networks form a new class of data-eﬃcient universal function approximators that naturally encode any underlying physical laws as prior information. In this ﬁrst part, we demonstrate how these networks can be used to infer solutions to partial diﬀerential equations, and obtain physics-informed surrogate models that are fully diﬀerentiable with respect to all input coordinates and free parameters.},
	language = {en},
	urldate = {2023-03-19},
	publisher = {arXiv},
	author = {Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},
	month = nov,
	year = {2017},
	note = {arXiv:1711.10561 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Mathematics - Dynamical Systems},
	file = {Raissi et al. - 2017 - Physics Informed Deep Learning (Part I) Data-driv.pdf:/Users/lucas/Zotero/storage/HAHQ8S8E/Raissi et al. - 2017 - Physics Informed Deep Learning (Part I) Data-driv.pdf:application/pdf;Raissi et al. - 2017 - Physics Informed Deep Learning (Part I) Data-driv.pdf:/Users/lucas/Zotero/storage/FBRCX4AX/Raissi et al. - 2017 - Physics Informed Deep Learning (Part I) Data-driv.pdf:application/pdf},
}

@misc{raissi_physics_2017-1,
	title = {Physics {Informed} {Deep} {Learning} ({Part} {II}): {Data}-driven {Discovery} of {Nonlinear} {Partial} {Differential} {Equations}},
	shorttitle = {Physics {Informed} {Deep} {Learning} ({Part} {II})},
	url = {http://arxiv.org/abs/1711.10566},
	abstract = {We introduce physics informed neural networks – neural networks that are trained to solve supervised learning tasks while respecting any given law of physics described by general nonlinear partial diﬀerential equations. In this second part of our two-part treatise, we focus on the problem of data-driven discovery of partial diﬀerential equations. Depending on whether the available data is scattered in space-time or arranged in ﬁxed temporal snapshots, we introduce two main classes of algorithms, namely continuous time and discrete time models. The eﬀectiveness of our approach is demonstrated using a wide range of benchmark problems in mathematical physics, including conservation laws, incompressible ﬂuid ﬂow, and the propagation of nonlinear shallow-water waves.},
	language = {en},
	urldate = {2023-03-19},
	publisher = {arXiv},
	author = {Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},
	month = nov,
	year = {2017},
	note = {arXiv:1711.10566 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Mathematics - Analysis of PDEs},
	file = {Raissi et al. - 2017 - Physics Informed Deep Learning (Part II) Data-dri.pdf:/Users/lucas/Zotero/storage/JSZGNEIV/Raissi et al. - 2017 - Physics Informed Deep Learning (Part II) Data-dri.pdf:application/pdf;Raissi et al. - 2017 - Physics Informed Deep Learning (Part II) Data-dri.pdf:/Users/lucas/Zotero/storage/WY2C77RF/Raissi et al. - 2017 - Physics Informed Deep Learning (Part II) Data-dri.pdf:application/pdf},
}

@article{lai_structural_2021,
	title = {Structural identification with physics-informed neural ordinary differential equations},
	volume = {508},
	issn = {0022460X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0022460X21002686},
	doi = {10.1016/j.jsv.2021.116196},
	language = {en},
	urldate = {2023-03-19},
	journal = {Journal of Sound and Vibration},
	author = {Lai, Zhilu and Mylonas, Charilaos and Nagarajaiah, Satish and Chatzi, Eleni},
	month = sep,
	year = {2021},
	pages = {116196},
	file = {Lai et al. - 2021 - Structural identification with physics-informed ne.pdf:/Users/lucas/Zotero/storage/26MCJ58C/Lai et al. - 2021 - Structural identification with physics-informed ne.pdf:application/pdf},
}

@article{sun_surrogate_2020,
	title = {Surrogate modeling for fluid flows based on physics-constrained deep learning without simulation data},
	volume = {361},
	issn = {00457825},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S004578251930622X},
	doi = {10.1016/j.cma.2019.112732},
	abstract = {Numerical simulations on fluid dynamics problems primarily rely on spatially or/and temporally discretization of the governing equation using polynomials into a finite-dimensional algebraic system. Due to the multi-scale nature of the physics and sensitivity from meshing a complicated geometry, such process can be computational prohibitive for most real-time applications (e.g., clinical diagnosis and surgery planning) and many-query analyses (e.g., optimization design and uncertainty quantification). Therefore, developing a cost-effective surrogate model is of great practical significance. Deep learning (DL) has shown new promises for surrogate modeling due to its capability of handling strong nonlinearity and high dimensionality. However, the off-the-shelf DL architectures, success of which heavily relies on the large amount of training data and interpolatory nature of the problem, fail to operate when the data becomes sparse. Unfortunately, data is often insufficient in most parametric fluid dynamics problems since each data point in the parameter space requires an expensive numerical simulation based on the first principle, e.g., Navier–Stokes equations. In this paper, we provide a physics-constrained DL approach for surrogate modeling of fluid flows without relying on any simulation data. Specifically, a structured deep neural network (DNN) architecture is devised to enforce the initial and boundary conditions, and the governing partial differential equations (i.e., Navier–Stokes equations) are incorporated into the loss of the DNN to drive the training. Numerical experiments are conducted on a number of internal flows relevant to hemodynamics applications, and the forward propagation of uncertainties in fluid properties and domain geometry is studied as well. The results show excellent agreement on the flow field and forward-propagated uncertainties between the DL surrogate approximations and the first-principle numerical simulations.},
	language = {en},
	urldate = {2023-03-19},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Sun, Luning and Gao, Han and Pan, Shaowu and Wang, Jian-Xun},
	month = apr,
	year = {2020},
	pages = {112732},
	file = {Sun et al. - 2020 - Surrogate modeling for fluid flows based on physic.pdf:/Users/lucas/Zotero/storage/NQ69SC65/Sun et al. - 2020 - Surrogate modeling for fluid flows based on physic.pdf:application/pdf},
}

@article{li_physics-guided_2021,
	title = {A physics-guided neural network framework for elastic plates: {Comparison} of governing equations-based and energy-based approaches},
	volume = {383},
	issn = {00457825},
	shorttitle = {A physics-guided neural network framework for elastic plates},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S004578252100270X},
	doi = {10.1016/j.cma.2021.113933},
	language = {en},
	urldate = {2023-03-19},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Li, Wei and Bazant, Martin Z. and Zhu, Juner},
	month = sep,
	year = {2021},
	pages = {113933},
	file = {Li et al. - 2021 - A physics-guided neural network framework for elas.pdf:/Users/lucas/Zotero/storage/R2VGV8PW/Li et al. - 2021 - A physics-guided neural network framework for elas.pdf:application/pdf},
}

@article{waheed_pinneik_2021,
	title = {{PINNeik}: {Eikonal} solution using physics-informed neural networks},
	volume = {155},
	issn = {00983004},
	shorttitle = {{PINNeik}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S009830042100131X},
	doi = {10.1016/j.cageo.2021.104833},
	abstract = {The eikonal equation is utilized across a wide spectrum of science and engineering disciplines. In seismology, it regulates seismic wave traveltimes needed for applications like source localization, imaging, and inversion. Several numerical algorithms have been developed over the years to solve the eikonal equation. However, these methods require considerable modifications to incorporate additional physics, such as anisotropy, and may even breakdown for certain complex forms of the eikonal equation, requiring approximation methods. Moreover, they suffer from computational bottleneck when repeated computations are needed for perturbations in the velocity model and/or the source location, particularly in large 3D models. Here, we propose an algorithm to solve the eikonal equation based on the emerging paradigm of physics-informed neural networks (PINNs). By minimizing a loss function formed by imposing the eikonal equation, we train a neural network to output traveltimes that are consistent with the underlying partial differential equation. We observe sufficiently high traveltime accuracy for most applications of interest. We also demonstrate how the proposed algorithm harnesses machine learning techniques like transfer learning and surrogate modeling to speed up traveltime computations for updated ve­ locity models and source locations. Furthermore, we use a locally adaptive activation function and adaptive weighting of the terms in the loss function to improve convergence rate and solution accuracy. We also show the flexibility of the method in incorporating medium anisotropy and free-surface topography compared to con­ ventional methods that require significant algorithmic modifications. These properties of the proposed PINN eikonal solver are highly desirable in obtaining a flexible and efficient forward modeling engine for seismological applications.},
	language = {en},
	urldate = {2023-03-19},
	journal = {Computers \& Geosciences},
	author = {Waheed, Umair bin and Haghighat, Ehsan and Alkhalifah, Tariq and Song, Chao and Hao, Qi},
	month = oct,
	year = {2021},
	pages = {104833},
	file = {Waheed et al. - 2021 - PINNeik Eikonal solution using physics-informed n.pdf:/Users/lucas/Zotero/storage/73D4PV6S/Waheed et al. - 2021 - PINNeik Eikonal solution using physics-informed n.pdf:application/pdf},
}

@article{goswami_transfer_2020,
	title = {Transfer learning enhanced physics informed neural network for phase-field modeling of fracture},
	volume = {106},
	issn = {01678442},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S016784421930357X},
	doi = {10.1016/j.tafmec.2019.102447},
	abstract = {In this work, we present a new physics informed neural network (PINN) algorithm for solving brittle fracture problems. While most of the PINN algorithms available in the literature minimize the residual of the governing partial differential equation, the proposed approach takes a different path by minimizing the variational energy of the system. Additionally, we modify the neural network output such that the boundary conditions associated with the problem are exactly satisfied. Compared to the conventional residual based PINN, the proposed approach has two major advantages. First, the imposition of boundary conditions is relatively simpler and more robust. Second, the order of derivatives present in the functional form of the variational energy is of lower order than in the residual form used in conventional PINN and hence, training the network is faster. To compute the total variational energy of the system, an efficient scheme that takes as input a geometry described by spline based CAD model and employs Gauss quadrature rules for numerical integration, has been proposed. Moreover, we note that for obtaining the crack path, the proposed PINN has to be trained at each load/displacement step, which can potentially make the algorithm computationally inefficient. To address this issue, we propose to use the concept ‘transfer learning’ wherein, instead of re-training the complete network, we only re-train the network partially while keeping the weights and the biases corresponding to the other portions fixed. With this setup, the computational efficiency of the proposed approach is significantly enhanced. The proposed approach is used to solve six fracture mechanics problems. For all the examples, results obtained using the proposed approach match closely with the results available in the literature. For the first two examples, we compare the results obtained using the proposed approach with the conventional residual based neural network results. For both the problems, the proposed approach is found to yield better accuracy compared to conventional residual based PINN algorithms.},
	language = {en},
	urldate = {2023-03-19},
	journal = {Theoretical and Applied Fracture Mechanics},
	author = {Goswami, Somdatta and Anitescu, Cosmin and Chakraborty, Souvik and Rabczuk, Timon},
	month = apr,
	year = {2020},
	pages = {102447},
	file = {Goswami et al. - 2020 - Transfer learning enhanced physics informed neural.pdf:/Users/lucas/Zotero/storage/RP62LA5H/Goswami et al. - 2020 - Transfer learning enhanced physics informed neural.pdf:application/pdf},
}

@article{yang_adversarial_2019,
	title = {Adversarial uncertainty quantification in physics-informed neural networks},
	volume = {394},
	issn = {00219991},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0021999119303584},
	doi = {10.1016/j.jcp.2019.05.027},
	language = {en},
	urldate = {2023-03-19},
	journal = {Journal of Computational Physics},
	author = {Yang, Yibo and Perdikaris, Paris},
	month = oct,
	year = {2019},
	pages = {136--152},
	file = {Yang et Perdikaris - 2019 - Adversarial uncertainty quantification in physics-.pdf:/Users/lucas/Zotero/storage/X3YPFKHA/Yang et Perdikaris - 2019 - Adversarial uncertainty quantification in physics-.pdf:application/pdf},
}

@article{yang_b-pinns_2021,
	title = {B-{PINNs}: {Bayesian} physics-informed neural networks for forward and inverse {PDE} problems with noisy data},
	volume = {425},
	issn = {00219991},
	shorttitle = {B-{PINNs}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0021999120306872},
	doi = {10.1016/j.jcp.2020.109913},
	language = {en},
	urldate = {2023-03-19},
	journal = {Journal of Computational Physics},
	author = {Yang, Liu and Meng, Xuhui and Karniadakis, George Em},
	month = jan,
	year = {2021},
	pages = {109913},
	file = {Yang et al. - 2021 - B-PINNs Bayesian physics-informed neural networks.pdf:/Users/lucas/Zotero/storage/QPFF7N9Z/Yang et al. - 2021 - B-PINNs Bayesian physics-informed neural networks.pdf:application/pdf},
}

@article{cai_deepmmnet_2021,
	title = {{DeepM}\&{Mnet}: {Inferring} the electroconvection multiphysics fields based on operator approximation by neural networks},
	volume = {436},
	issn = {00219991},
	shorttitle = {{DeepM}\&{Mnet}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0021999121001911},
	doi = {10.1016/j.jcp.2021.110296},
	language = {en},
	urldate = {2023-03-19},
	journal = {Journal of Computational Physics},
	author = {Cai, Shengze and Wang, Zhicheng and Lu, Lu and Zaki, Tamer A. and Karniadakis, George Em},
	month = jul,
	year = {2021},
	pages = {110296},
	file = {Cai et al. - 2021 - DeepM&Mnet Inferring the electroconvection multip.pdf:/Users/lucas/Zotero/storage/FGL4PLGW/Cai et al. - 2021 - DeepM&Mnet Inferring the electroconvection multip.pdf:application/pdf},
}

@article{ramabathiran_spinn_2021,
	title = {{SPINN}: {Sparse}, {Physics}-based, and partially {Interpretable} {Neural} {Networks} for {PDEs}},
	volume = {445},
	issn = {00219991},
	shorttitle = {{SPINN}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0021999121004952},
	doi = {10.1016/j.jcp.2021.110600},
	language = {en},
	urldate = {2023-03-19},
	journal = {Journal of Computational Physics},
	author = {Ramabathiran, Amuthan A. and Ramachandran, Prabhu},
	month = nov,
	year = {2021},
	pages = {110600},
	file = {Ramabathiran et Ramachandran - 2021 - SPINN Sparse, Physics-based, and partially Interp.pdf:/Users/lucas/Zotero/storage/QJ4W54AR/Ramabathiran et Ramachandran - 2021 - SPINN Sparse, Physics-based, and partially Interp.pdf:application/pdf},
}

@article{mishra_physics_2021,
	title = {Physics informed neural networks for simulating radiative transfer},
	volume = {270},
	issn = {00224073},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0022407321001989},
	doi = {10.1016/j.jqsrt.2021.107705},
	language = {en},
	urldate = {2023-03-19},
	journal = {Journal of Quantitative Spectroscopy and Radiative Transfer},
	author = {Mishra, Siddhartha and Molinaro, Roberto},
	month = aug,
	year = {2021},
	pages = {107705},
	file = {Mishra et Molinaro - 2021 - Physics informed neural networks for simulating ra.pdf:/Users/lucas/Zotero/storage/MAFC66S6/Mishra et Molinaro - 2021 - Physics informed neural networks for simulating ra.pdf:application/pdf},
}

@article{kharazmi_hp-vpinns_2021,
	title = {hp-{VPINNs}: {Variational} physics-informed neural networks with domain decomposition},
	volume = {374},
	issn = {00457825},
	shorttitle = {hp-{VPINNs}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0045782520307325},
	doi = {10.1016/j.cma.2020.113547},
	abstract = {We formulate a general framework for hp-variational physics-informed neural networks (hp-VPINNs) based on the nonlinear approximation of shallow and deep neural networks and hp-refinement via domain decomposition and projection onto the space of high-order polynomials. The trial space is the space of neural network, which is defined globally over the entire computational domain, while the test space contains piecewise polynomials. Specifically in this study, the hp-refinement corresponds to a global approximation with a local learning algorithm that can efficiently localize the network parameter optimization. We demonstrate the advantages of hp-VPINNs in both accuracy and training cost for several numerical examples of function approximation and in solving differential equations.},
	language = {en},
	urldate = {2023-03-19},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Kharazmi, Ehsan and Zhang, Zhongqiang and Karniadakis, George E.M.},
	month = feb,
	year = {2021},
	pages = {113547},
	file = {Kharazmi et al. - 2021 - hp-VPINNs Variational physics-informed neural net.pdf:/Users/lucas/Zotero/storage/MPFXKU2N/Kharazmi et al. - 2021 - hp-VPINNs Variational physics-informed neural net.pdf:application/pdf},
}

@article{viana_estimating_2021,
	title = {Estimating model inadequacy in ordinary differential equations with physics-informed neural networks},
	volume = {245},
	issn = {00457949},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0045794920302613},
	doi = {10.1016/j.compstruc.2020.106458},
	abstract = {A number of physical systems can be described by ordinary differential equations. When physics is well understood, the time dependent responses are easily obtained numerically. The particular numerical method used for integration depends on the application. Unfortunately, when physics is not fully understood, the discrepancies between predictions and observed responses can be large and unacceptable. In this paper, we propose an approach that uses observed data to estimate the missing physics in the original model (i.e., model-form uncertainty). In our approach, we ﬁrst design recurrent neural networks to perform numerical integration of the ordinary differential equations. Then, we implement the recurrent neural network as a directed graph. This way, the nodes in the graph represent the physics-informed kernels found in the ordinary differential equations. We quantify the missing physics by carefully introducing data-driven in the directed graph. This allows us to estimate the missing physics (discrepancy term) even for hidden nodes of the graph. We studied the performance of our proposed approach with the aid of three case studies (fatigue crack growth, corrosion-fatigue crack growth, and bearing fatigue) and stateof-the-art machine learning software packages. Our results demonstrate the ability to perform estimation of discrepancy, reducing gap between predictions and observations, at reasonable computational cost.},
	language = {en},
	urldate = {2023-03-19},
	journal = {Computers \& Structures},
	author = {Viana, Felipe A.C. and Nascimento, Renato G. and Dourado, Arinan and Yucesan, Yigit A.},
	month = mar,
	year = {2021},
	pages = {106458},
	file = {Viana et al. - 2021 - Estimating model inadequacy in ordinary differenti.pdf:/Users/lucas/Zotero/storage/W2KGMVNL/Viana et al. - 2021 - Estimating model inadequacy in ordinary differenti.pdf:application/pdf},
}

@article{nascimento_tutorial_2020,
	title = {A tutorial on solving ordinary differential equations using {Python} and hybrid physics-informed neural network},
	volume = {96},
	issn = {09521976},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S095219762030292X},
	doi = {10.1016/j.engappai.2020.103996},
	abstract = {We present a tutorial on how to directly implement integration of ordinary differential equations through recurrent neural networks using Python. In order to simplify the implementation, we leveraged modern machine learning frameworks such as TensorFlow and Keras. Besides, offering implementation of basic models (such as multilayer perceptrons and recurrent neural networks) and optimization methods, these frameworks offer powerful automatic differentiation. With all that, the main advantage of our approach is that one can implement hybrid models combining physics-informed and data-driven kernels, where data-driven kernels are used to reduce the gap between predictions and observations. Alternatively, we can also perform model parameter identification. In order to illustrate our approach, we used two case studies. The first one consisted of performing fatigue crack growth integration through Euler’s forward method using a hybrid model combining a data-driven stress intensity range model with a physics-based crack length increment model. The second case study consisted of performing model parameter identification of a dynamic two-degree-of-freedom system through Runge–Kutta integration. The examples presented here as well as source codes are all open-source under the GitHub repository https://github.com/PML-UCF/pinn\_code\_tutorial.},
	language = {en},
	urldate = {2023-03-19},
	journal = {Engineering Applications of Artificial Intelligence},
	author = {Nascimento, Renato G. and Fricke, Kajetan and Viana, Felipe A.C.},
	month = nov,
	year = {2020},
	pages = {103996},
	file = {Nascimento et al. - 2020 - A tutorial on solving ordinary differential equati.pdf:/Users/lucas/Zotero/storage/5YGZMMQ4/Nascimento et al. - 2020 - A tutorial on solving ordinary differential equati.pdf:application/pdf},
}

@article{he_physics-informed_2020,
	title = {Physics-informed neural networks for multiphysics data assimilation with application to subsurface transport},
	volume = {141},
	issn = {03091708},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0309170819311649},
	doi = {10.1016/j.advwatres.2020.103610},
	language = {en},
	urldate = {2023-03-19},
	journal = {Advances in Water Resources},
	author = {He, QiZhi and Barajas-Solano, David and Tartakovsky, Guzel and Tartakovsky, Alexandre M.},
	month = jul,
	year = {2020},
	pages = {103610},
	file = {He et al. - 2020 - Physics-informed neural networks for multiphysics .pdf:/Users/lucas/Zotero/storage/TD399YYH/He et al. - 2020 - Physics-informed neural networks for multiphysics .pdf:application/pdf},
}

@article{de_ryck_approximation_2021,
	title = {On the approximation of functions by tanh neural networks},
	volume = {143},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608021003208},
	doi = {10.1016/j.neunet.2021.08.015},
	abstract = {We derive bounds on the error, in high-order Sobolev norms, incurred in the approximation of Sobolev-regular as well as analytic functions by neural networks with the hyperbolic tangent activation function. These bounds provide explicit estimates on the approximation error with respect to the size of the neural networks. We show that tanh neural networks with only two hidden layers suffice to approximate functions at comparable or better rates than much deeper ReLU neural networks.},
	language = {en},
	urldate = {2023-03-19},
	journal = {Neural Networks},
	author = {De Ryck, Tim and Lanthaler, Samuel and Mishra, Siddhartha},
	month = nov,
	year = {2021},
	pages = {732--750},
	file = {De Ryck et al. - 2021 - On the approximation of functions by tanh neural n.pdf:/Users/lucas/Zotero/storage/TXTPU8BZ/De Ryck et al. - 2021 - On the approximation of functions by tanh neural n.pdf:application/pdf},
}

@misc{kutyniok_mathematics_2022,
	title = {The {Mathematics} of {Artificial} {Intelligence}},
	url = {http://arxiv.org/abs/2203.08890},
	abstract = {We currently witness the spectacular success of artiﬁcial intelligence in both science and public life. However, the development of a rigorous mathematical foundation is still at an early stage. In this survey article, which is based on an invited lecture at the International Congress of Mathematicians 2022, we will in particular focus on the current “workhorse” of artiﬁcial intelligence, namely deep neural networks. We will present the main theoretical directions along with several exemplary results and discuss key open problems.},
	language = {en},
	urldate = {2023-03-19},
	publisher = {arXiv},
	author = {Kutyniok, Gitta},
	month = mar,
	year = {2022},
	note = {arXiv:2203.08890 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - History and Overview, Primary 68T07, Secondary 41A25, 42C15, 35C20, 65D18},
	annote = {Comment: 16 pages, 7 figures},
	file = {Kutyniok - 2022 - The Mathematics of Artificial Intelligence.pdf:/Users/lucas/Zotero/storage/72T84P29/Kutyniok - 2022 - The Mathematics of Artificial Intelligence.pdf:application/pdf},
}

@misc{de_ryck_error_2023,
	title = {Error estimates for physics informed neural networks approximating the {Navier}-{Stokes} equations},
	url = {http://arxiv.org/abs/2203.09346},
	abstract = {We prove rigorous bounds on the errors resulting from the approximation of the incompressible Navier-Stokes equations with (extended) physics-informed neural networks. We show that the underlying PDE residual can be made arbitrarily small for tanh neural networks with two hidden layers. Moreover, the total error can be estimated in terms of the training error, network size and number of quadrature points. The theory is illustrated with numerical experiments.},
	language = {en},
	urldate = {2023-03-19},
	publisher = {arXiv},
	author = {De Ryck, Tim and Jagtap, Ameya D. and Mishra, Siddhartha},
	month = feb,
	year = {2023},
	note = {arXiv:2203.09346 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis},
	file = {De Ryck et al. - 2023 - Error estimates for physics informed neural networ.pdf:/Users/lucas/Zotero/storage/NNTV9IP2/De Ryck et al. - 2023 - Error estimates for physics informed neural networ.pdf:application/pdf},
}

@article{lin_seamless_2021,
	title = {A seamless multiscale operator neural network for inferring bubble dynamics},
	volume = {929},
	issn = {0022-1120, 1469-7645},
	url = {https://www.cambridge.org/core/product/identifier/S0022112021008661/type/journal_article},
	doi = {10.1017/jfm.2021.866},
	abstract = {Modelling multiscale systems from nanoscale to macroscale requires the use of atomistic and continuum methods and, correspondingly, different computer codes. Here, we develop a seamless method based on DeepONet, which is a composite deep neural network (a branch and a trunk network) for regressing operators. In particular, we consider bubble growth dynamics, and we model tiny bubbles of initial size from 100 nm to 10 μm, modelled by the Rayleigh–Plesset equation in the continuum regime above 1 μm and the dissipative particle dynamics method for bubbles below 1 μm in the atomistic regime. After an ofﬂine training based on data from both regimes, DeepONet can make accurate predictions of bubble growth on-the-ﬂy (within a fraction of a second) across four orders of magnitude difference in spatial scales and two orders of magnitude in temporal scales. The framework of DeepONet is general and can be used for unifying physical models of different scales in diverse multiscale applications.},
	language = {en},
	urldate = {2023-03-19},
	journal = {Journal of Fluid Mechanics},
	author = {Lin, Chensen and Maxey, Martin and Li, Zhen and Karniadakis, George Em},
	month = dec,
	year = {2021},
	pages = {A18},
	file = {Lin et al. - 2021 - A seamless multiscale operator neural network for .pdf:/Users/lucas/Zotero/storage/GTNFY35L/Lin et al. - 2021 - A seamless multiscale operator neural network for .pdf:application/pdf},
}

@article{pinkus_approximation_1999,
	title = {Approximation theory of the {MLP} model in neural networks},
	volume = {8},
	issn = {0962-4929, 1474-0508},
	url = {https://www.cambridge.org/core/product/identifier/S0962492900002919/type/journal_article},
	doi = {10.1017/S0962492900002919},
	abstract = {In this survey we discuss various approximation-theoretic problems that arise in the multilayer feedforward perceptron (MLP) model in neural networks. The MLP model is one of the more popular and practical of the many neural network models. Mathematically it is also one of the simpler models. Nonetheless the mathematics of this model is not well understood, and many of these problems are approximation-theoretic in character. Most of the research we will discuss is of very recent vintage. We will report on what has been done and on various unanswered questions. We will not be presenting practical (algorithmic) methods. We will, however, be exploring the capabilities and limitations of this model.},
	language = {en},
	urldate = {2023-03-19},
	journal = {Acta Numerica},
	author = {Pinkus, Allan},
	month = jan,
	year = {1999},
	pages = {143--195},
	file = {Pinkus - 1999 - Approximation theory of the MLP model in neural ne.pdf:/Users/lucas/Zotero/storage/TVC6GS9N/Pinkus - 1999 - Approximation theory of the MLP model in neural ne.pdf:application/pdf},
}

@article{elbrachter_deep_2021,
	title = {Deep {Neural} {Network} {Approximation} {Theory}},
	volume = {67},
	issn = {0018-9448, 1557-9654},
	url = {https://ieeexplore.ieee.org/document/9363169/},
	doi = {10.1109/TIT.2021.3062161},
	abstract = {This paper develops fundamental limits of deep neural network learning by characterizing what is possible if no constraints are imposed on the learning algorithm and on the amount of training data. Concretely, we consider Kolmogorovoptimal approximation through deep neural networks with the guiding theme being a relation between the complexity of the function (class) to be approximated and the complexity of the approximating network in terms of connectivity and memory requirements for storing the network topology and the associated quantized weights. The theory we develop establishes that deep networks are Kolmogorov-optimal approximants for markedly different function classes, such as unit balls in Besov spaces and modulation spaces. In addition, deep networks provide exponential approximation accuracy—i.e., the approximation error decays exponentially in the number of nonzero weights in the network—of the multiplication operation, polynomials, sinusoidal functions, and certain smooth functions. Moreover, this holds true even for one-dimensional oscillatory textures and the Weierstrass function—a fractal function, neither of which has previously known methods achieving exponential approximation accuracy. We also show that in the approximation of sufﬁciently smooth functions ﬁnite-width deep networks require strictly smaller connectivity than ﬁnite-depth wide networks.},
	language = {en},
	number = {5},
	urldate = {2023-03-19},
	journal = {IEEE Transactions on Information Theory},
	author = {Elbrachter, Dennis and Perekrestenko, Dmytro and Grohs, Philipp and Bolcskei, Helmut},
	month = may,
	year = {2021},
	pages = {2581--2623},
	file = {Elbrachter et al. - 2021 - Deep Neural Network Approximation Theory.pdf:/Users/lucas/Zotero/storage/JP27QK9W/Elbrachter et al. - 2021 - Deep Neural Network Approximation Theory.pdf:application/pdf},
}

@article{mishra_estimates_2023,
	title = {Estimates on the generalization error of physics-informed neural networks for approximating {PDEs}},
	volume = {43},
	issn = {0272-4979, 1464-3642},
	url = {https://academic.oup.com/imajna/article/43/1/1/6503953},
	doi = {10.1093/imanum/drab093},
	abstract = {Abstract
            Physics-informed neural networks (PINNs) have recently been widely used for robust and accurate approximation of partial differential equations (PDEs). We provide upper bounds on the generalization error of PINNs approximating solutions of the forward problem for PDEs. An abstract formalism is introduced and stability properties of the underlying PDE are leveraged to derive an estimate for the generalization error in terms of the training error and number of training samples. This abstract framework is illustrated with several examples of nonlinear PDEs. Numerical experiments, validating the proposed theory, are also presented.},
	language = {en},
	number = {1},
	urldate = {2023-03-19},
	journal = {IMA Journal of Numerical Analysis},
	author = {Mishra, Siddhartha and Molinaro, Roberto},
	month = feb,
	year = {2023},
	pages = {1--43},
	file = {Mishra et Molinaro - 2023 - Estimates on the generalization error of physics-i.pdf:/Users/lucas/Zotero/storage/PNVKTDP4/Mishra et Molinaro - 2023 - Estimates on the generalization error of physics-i.pdf:application/pdf},
}

@article{farrah_observational_2023,
	title = {Observational {Evidence} for {Cosmological} {Coupling} of {Black} {Holes} and its {Implications} for an {Astrophysical} {Source} of {Dark} {Energy}},
	volume = {944},
	issn = {2041-8205, 2041-8213},
	url = {https://iopscience.iop.org/article/10.3847/2041-8213/acb704},
	doi = {10.3847/2041-8213/acb704},
	abstract = {Observations have found black holes spanning 10 orders of magnitude in mass across most of cosmic history. The Kerr black hole solution is, however, provisional as its behavior at inﬁnity is incompatible with an expanding universe. Black hole models with realistic behavior at inﬁnity predict that the gravitating mass of a black hole can increase with the expansion of the universe independently of accretion or mergers, in a manner that depends on the black hole’s interior solution. We test this prediction by considering the growth of supermassive black holes in elliptical galaxies over 0 {\textless} z  2.5. We ﬁnd evidence for cosmologically coupled mass growth among these black holes, with zero cosmological coupling excluded at 99.98\% conﬁdence. The redshift dependence of the mass growth implies that, at z  7, black holes contribute an effectively constant cosmological energy density to Friedmann’s equations. The continuity equation then requires that black holes contribute cosmologically as vacuum energy. We further show that black hole production from the cosmic star formation history gives the value of ΩΛ measured by Planck while being consistent with constraints from massive compact halo objects. We thus propose that stellar remnant black holes are the astrophysical origin of dark energy, explaining the onset of accelerating expansion at z ∼ 0.7.},
	language = {en},
	number = {2},
	urldate = {2023-03-19},
	journal = {The Astrophysical Journal Letters},
	author = {Farrah, Duncan and Croker, Kevin S. and Zevin, Michael and Tarlé, Gregory and Faraoni, Valerio and Petty, Sara and Afonso, Jose and Fernandez, Nicolas and Nishimura, Kurtis A. and Pearson, Chris and Wang, Lingyu and Clements, David L and Efstathiou, Andreas and Hatziminaoglou, Evanthia and Lacy, Mark and McPartland, Conor and Pitchford, Lura K and Sakai, Nobuyuki and Weiner, Joel},
	month = feb,
	year = {2023},
	pages = {L31},
	file = {Farrah et al. - 2023 - Observational Evidence for Cosmological Coupling o.pdf:/Users/lucas/Zotero/storage/TRTED76B/Farrah et al. - 2023 - Observational Evidence for Cosmological Coupling o.pdf:application/pdf},
}

@article{berman_survey_2019,
	title = {A {Survey} of {Deep} {Learning} {Methods} for {Cyber} {Security}},
	volume = {10},
	issn = {2078-2489},
	url = {https://www.mdpi.com/2078-2489/10/4/122},
	doi = {10.3390/info10040122},
	abstract = {This survey paper describes a literature review of deep learning (DL) methods for cyber security applications. A short tutorial-style description of each DL method is provided, including deep autoencoders, restricted Boltzmann machines, recurrent neural networks, generative adversarial networks, and several others. Then we discuss how each of the DL methods is used for security applications. We cover a broad array of attack types including malware, spam, insider threats, network intrusions, false data injection, and malicious domain names used by botnets.},
	language = {en},
	number = {4},
	urldate = {2023-03-19},
	journal = {Information},
	author = {Berman, Daniel and Buczak, Anna and Chavis, Jeffrey and Corbett, Cherita},
	month = apr,
	year = {2019},
	pages = {122},
	file = {Berman et al. - 2019 - A Survey of Deep Learning Methods for Cyber Securi.pdf:/Users/lucas/Zotero/storage/S58TAJWV/Berman et al. - 2019 - A Survey of Deep Learning Methods for Cyber Securi.pdf:application/pdf},
}

@article{elmarakeby_biologically_2021,
	title = {Biologically informed deep neural network for prostate cancer discovery},
	volume = {598},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/s41586-021-03922-4},
	doi = {10.1038/s41586-021-03922-4},
	abstract = {Abstract
            
              The determination of molecular features that mediate clinically aggressive phenotypes in prostate cancer remains a major biological and clinical challenge
              1,2
              . Recent advances in interpretability of machine learning models as applied to biomedical problems may enable discovery and prediction in clinical cancer genomics
              3–5
              . Here we developed P-NET—a biologically informed deep learning model—to stratify patients with prostate cancer by treatment-resistance state and evaluate molecular drivers of treatment resistance for therapeutic targeting through complete model interpretability. We demonstrate that P-NET can predict cancer state using molecular data with a performance that is superior to other modelling approaches. Moreover, the biological interpretability within P-NET revealed established and novel molecularly altered candidates, such as
              MDM4
              and
              FGFR1
              , which were implicated in predicting advanced disease and validated in vitro. Broadly, biologically informed fully interpretable neural networks enable preclinical discovery and clinical prediction in prostate cancer and may have general applicability across cancer types.},
	language = {en},
	number = {7880},
	urldate = {2023-03-19},
	journal = {Nature},
	author = {Elmarakeby, Haitham A. and Hwang, Justin and Arafeh, Rand and Crowdis, Jett and Gang, Sydney and Liu, David and AlDubayan, Saud H. and Salari, Keyan and Kregel, Steven and Richter, Camden and Arnoff, Taylor E. and Park, Jihye and Hahn, William C. and Van Allen, Eliezer M.},
	month = oct,
	year = {2021},
	pages = {348--352},
	file = {Elmarakeby et al. - 2021 - Biologically informed deep neural network for pros.pdf:/Users/lucas/Zotero/storage/U2ZWJRK8/Elmarakeby et al. - 2021 - Biologically informed deep neural network for pros.pdf:application/pdf},
}

@article{tartakovsky_physicsinformed_2020,
	title = {Physics‐{Informed} {Deep} {Neural} {Networks} for {Learning} {Parameters} and {Constitutive} {Relationships} in {Subsurface} {Flow} {Problems}},
	volume = {56},
	issn = {0043-1397, 1944-7973},
	url = {https://onlinelibrary.wiley.com/doi/10.1029/2019WR026731},
	doi = {10.1029/2019WR026731},
	abstract = {We present a physics-informed deep neural network (DNN) method for estimating hydraulic conductivity in saturated and unsaturated flows governed by Darcy's law. For saturated flow, we approximate hydraulic conductivity and head with two DNNs and use Darcy's law in addition to measurements of hydraulic conductivity and head to train these DNNs. For unsaturated flow, we approximate unsaturated conductivity function and capillary pressure with DNNs and train these DNNs using measurements of capillary pressure and the Richards equation. Because it is difficult to measure unsaturated conductivity in the field, we assume that no measurements of unsaturated conductivity are available. The proposed approach enforces the partial differential equation (PDE) (Darcy or Richards equation) constraints by minimizing the PDE residual at select points in the simulation domain. We demonstrate that physics constraints increase the accuracy of DNN approximations of sparsely observed functions and allow for training DNNs when no direct measurements of the functions of interest are available. For the saturated conductivity estimation problem, we show that the physics-informed DNN method is more accurate than the state-of-the-art maximum a posteriori probability method. For the unsaturated flow in homogeneous porous media, we find that the proposed method can accurately estimate the pressure-conductivity relationship based on the capillary pressure measurements only, even in the presence of measurement noise.},
	language = {en},
	number = {5},
	urldate = {2023-03-19},
	journal = {Water Resources Research},
	author = {Tartakovsky, A. M. and Marrero, C. Ortiz and Perdikaris, Paris and Tartakovsky, G. D. and Barajas‐Solano, D.},
	month = may,
	year = {2020},
	file = {Tartakovsky et al. - 2020 - Physics‐Informed Deep Neural Networks for Learning.pdf:/Users/lucas/Zotero/storage/9V2QM9RI/Tartakovsky et al. - 2020 - Physics‐Informed Deep Neural Networks for Learning.pdf:application/pdf},
}

@article{cheng_deep_2021,
	title = {Deep {Learning} {Method} {Based} on {Physics} {Informed} {Neural} {Network} with {Resnet} {Block} for {Solving} {Fluid} {Flow} {Problems}},
	volume = {13},
	issn = {2073-4441},
	url = {https://www.mdpi.com/2073-4441/13/4/423},
	doi = {10.3390/w13040423},
	abstract = {Solving ﬂuid dynamics problems mainly rely on experimental methods and numerical simulation. However, in experimental methods it is difﬁcult to simulate the physical problems in reality, and there is also a high-cost to the economy while numerical simulation methods are sensitive about meshing a complicated structure. It is also time-consuming due to the billion degrees of freedom in relevant spatial-temporal ﬂow ﬁelds. Therefore, constructing a cost-effective model to settle ﬂuid dynamics problems is of signiﬁcant meaning. Deep learning (DL) has great abilities to handle strong nonlinearity and high dimensionality that attracts much attention for solving ﬂuid problems. Unfortunately, the proposed surrogate models in DL are almost black-box models and lack interpretation. In this paper, the Physical Informed Neural Network (PINN) combined with Resnet blocks is proposed to solve ﬂuid ﬂows depending on the partial differential equations (i.e., NavierStokes equation) which are embedded into the loss function of the deep neural network to drive the model. In addition, the initial conditions and boundary conditions are also considered in the loss function. To validate the performance of the PINN with Resnet blocks, Burger’s equation with a discontinuous solution and Navier-Stokes (N-S) equation with continuous solution are selected. The results show that the PINN with Resnet blocks (Res-PINN) has stronger predictive ability than traditional deep learning methods. In addition, the Res-PINN can predict the whole velocity ﬁelds and pressure ﬁelds in spatial-temporal ﬂuid ﬂows, the magnitude of the mean square error of the ﬂuid ﬂow reaches to 10−5. The inverse problems of the ﬂuid ﬂows are also well conducted. The errors of the inverse parameters are 0.98\% and 3.1\% in clean data and 0.99\% and 3.1\% in noisy data.},
	language = {en},
	number = {4},
	urldate = {2023-03-19},
	journal = {Water},
	author = {Cheng, Chen and Zhang, Guang-Tao},
	month = feb,
	year = {2021},
	pages = {423},
	file = {Cheng et Zhang - 2021 - Deep Learning Method Based on Physics Informed Neu.pdf:/Users/lucas/Zotero/storage/A4XP5VX9/Cheng et Zhang - 2021 - Deep Learning Method Based on Physics Informed Neu.pdf:application/pdf},
}
