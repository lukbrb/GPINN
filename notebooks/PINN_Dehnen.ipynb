{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18a22a5b-f3cb-4f20-adc6-bd7168bd74a6",
   "metadata": {},
   "source": [
    "# PINN Solver for Dehnen profile\n",
    "\n",
    "## 1. Introduction\n",
    "We start from the Dehnen density profile $$\\rho(r) = \\dfrac{(3-\\gamma)M}{4\\pi}\\dfrac{a}{r^{\\gamma}(r+a)^{4-\\gamma}} $$\n",
    "For $\\gamma=1$ and $\\gamma=2$ we recover the Hernquist and Jaffe profiles, respectively. Note that the parameter $\\gamma$ is restriced to the interval $[0, 3)$.\n",
    "\n",
    "We note that it is convenient to use the expression of Dehnen density profile given in Binnet et ... :\n",
    "\n",
    "$$\\rho(r) = \\dfrac{\\rho_0}{\\left(\\frac{r}{a}\\right)^{\\gamma}\\left(1+\\frac{r}{a}\\right)^{4-\\gamma}} = \\dfrac{M}{2\\pi a^3}\\dfrac{1}{\\left(\\frac{r}{a}\\right)^{\\gamma}\\left(1+\\frac{r}{a}\\right)^{4-\\gamma}}$$  as $M=2\\pi a^3 \\rho_0$\n",
    "\n",
    "### 1.1 The equation to solve\n",
    "For such a potential, the Poisson equation reads:\n",
    "$$\\nabla ^2 \\phi = 4\\pi G \\rho$$\n",
    "\n",
    "$$ \\Leftrightarrow \\dfrac{1}{r^2} \\dfrac{\\partial}{\\partial r}\\left(r^2 \\dfrac{\\partial \\phi}{\\partial r}\\right) = 4\\pi G \\left[\\dfrac{M}{2\\pi a^3}\\dfrac{1}{\\left(\\frac{r}{a}\\right)^{\\gamma}\\left(1+\\frac{r}{a}\\right)^{4-\\gamma}}\\right]$$ \n",
    "\n",
    "Given that the density-potential pair depends solely on the radial coordinate $r$, the equation reduces to\n",
    "\n",
    "$$  \\dfrac{1}{r^2} \\dfrac{d}{d r}\\left(r^2 \\dfrac{d \\phi}{d r}\\right) = \\dfrac{2GM}{a^3\\left(\\frac{r}{a}\\right)^{\\gamma}\\left(1+\\frac{r}{a}\\right)^{4-\\gamma}}$$\n",
    "\n",
    "\n",
    "Setting $s \\rightarrow \\frac{r}{a}$, we get\n",
    "$$  \\dfrac{1}{a^2 s^2}\\dfrac{d}{a ds}\\left(a^2 s^2 \\dfrac{d \\phi}{a ds}\\right) = \\dfrac{2GM}{a^3 s^{\\gamma}(1+s)^{4-\\gamma}}$$\n",
    "\n",
    "\n",
    "Finally, in the case of the Dehnen profile, the Poisson equation can be written such as:\n",
    "\n",
    "$$ \\boxed{\\dfrac{d}{d r}\\left(r^2 \\dfrac{d \\phi}{d r}\\right) = \\dfrac{2r^{2-\\gamma}}{(1+r)^{4-\\gamma}}}$$\n",
    "\n",
    "where we have set $\\frac{GM}{a}$ to unity and changed back $s$ to $r$.\n",
    "\n",
    "The potential can be computed analytically and is given by:\n",
    "\n",
    "$$ \\boxed{ \\Phi(s) = -\\dfrac{1}{2 - \\gamma} \\left[1 - \\left( \\dfrac{s}{1 + s} \\right )^{2-\\gamma}\\right]}$$\n",
    "\n",
    "and in the particular case $\\gamma = 2$, this is Jaffe's model with :\n",
    "\n",
    "$$\\boxed{\\Phi(s) = \\ln\\left( \\dfrac{s}{1 + s}\\right)}$$\n",
    "\n",
    "### 1.2 Initial conditions\n",
    "\n",
    "Given the expression of the potential, the training domain cannot start at $s=0$. It would diverge for $\\gamma=2$. We therefore have to give it a small \n",
    "\n",
    "## 2. Structure of the PINN\n",
    "\n",
    "Whereas for the Hernquist profile we used the neural network as an approximation of a function $f$ taking as an input only spatial coordinates $r$, we want now the network to take an extra parameter $\\gamma$ such as $f(r, \\gamma)$.\n",
    "\n",
    "**Loss function for the Poisson equation**\n",
    "\n",
    "In our case, we do not consider any data points. Therefore $N_d$ = 0 and $\\mathcal{L}_{data}(\\theta)=0$. Also we consider for now that $\\omega_{\\mathcal{F}} = \\omega_{\\mathcal{B}} = 1$. Finally, we consider in the training a 1-D domain, with $N_c = 1000$ and $N_B = 1$.\n",
    "\n",
    "We note that here, the residual $r_{\\theta} (z)$ is as follow :\n",
    "\n",
    "$$ r_{\\theta} (z) = \\dfrac{d}{d r}\\left(r^2 \\dfrac{d \\phi}{d r}\\right) - \\dfrac{2r^{2-\\gamma}}{(1+r)^{4-\\gamma}}$$\n",
    "\n",
    "Therefore, we want to minimize the following loss function :\n",
    "\n",
    "$$ \\mathcal{L}(\\theta) = \\dfrac{1}{N_c}\\sum^{N_c}_{i=1} \\left\\|\\dfrac{df}{d r} - \\dfrac{2r^{2-\\gamma}}{(1+r)^{4-\\gamma}} - r_i \\right\\|^2 + \\dfrac{1}{N_B}\\sum^{N_B}_{i=1} \\left\\|\\hat{u}(z_0) - u_0\\right\\|^2$$ \n",
    "\n",
    "where we set $f = r^2 \\dfrac{d \\phi}{d r}$. \n",
    "\n",
    "### 2.1 PINN input\n",
    "\n",
    "The PINN has to be able to learn over a wide dynamic range. Where at large radii, the potential vanishes, at small radii the behaviour strongly depends on the value of $\\gamma$. In order for the PINN to learn this behaviour, we now try to give as input a grid $\\gamma \\in [0, 3) \\times [x_i, x_f]$ where $x_i$ and $x_f$ are the initial and final points of the spatial domain. In this configuration, the network will see the behaviour of the potnetial for each $\\gamma$, and at each value of $x_n$.\n",
    "\n",
    "### 2.2 PINN output\n",
    "\n",
    "The output of the PINN is $f(x_n, \\gamma_n)$, the approximation of the gravitational potential at any point $x_n$ for a given value $\\gamma_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa25b149-d9a6-4a1f-b40e-baa86b2e1f81",
   "metadata": {},
   "source": [
    "https://pytorch.org/docs/stable/generated/torch.nn.SiLU.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4bd95e-c0b8-4d57-9682-30c95714b429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from pyDOE import lhs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc41500f-3e53-46f9-8605-9dd00a3f16e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dehnen(radius, gamma, scale_factor=1):\n",
    "    \"\"\" Value of the gravitational potential\n",
    "        in the case of a Dehnen profile\n",
    "    \"\"\"\n",
    "    #if gamma == 2:\n",
    "        #return np.log(radius / (radius + scale_factor))\n",
    "    power1 = 2 - gamma\n",
    "    return -1 / power1 * (1 - (radius / (radius + scale_factor)) ** power1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9de2eb-fe9a-4f23-9254-3a2d24833b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 50_000\n",
    "lr = 1e-3\n",
    "layers = np.array([2,32,32,1]) # hidden layers\n",
    "# To generate new data:\n",
    "x_min = 1e-2\n",
    "x_max = 10\n",
    "gamma_min = 0\n",
    "gamma_max = 2.99\n",
    "total_points_x = 200\n",
    "total_points_gamma = 100\n",
    "#Nu: Number of training points # Nf: Number of collocation points (Evaluate PDE)\n",
    "Nu = 100\n",
    "Nf = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25467ec-71be-4478-bd7b-eb84a4f9591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN(nn.Module):\n",
    "    # https://github.com/omniscientoctopus/Physics-Informed-Neural-Networks/tree/main/PyTorch/Burgers'%20Equation\n",
    "    # Neural Network\n",
    "    def __init__(self, _layers):\n",
    "        super().__init__()  # call __init__ from parent class\n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction='mean')\n",
    "        self.linears = nn.ModuleList([nn.Linear(_layers[i], _layers[i + 1]) for i in range(len(_layers) - 1)])\n",
    "        self.iter = 0  # For the Optimizer\n",
    "        for i in range(len(_layers) - 1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "\n",
    "    def forward(self, _x):\n",
    "        if not torch.is_tensor(_x):\n",
    "            _x = torch.from_numpy(_x)\n",
    "        a = _x.float()\n",
    "        for i in range(len(layers) - 2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z)\n",
    "        a = self.linears[-1](a)\n",
    "        return a\n",
    "\n",
    "    def lossBC(self, x_BC, y_BC):\n",
    "        loss_BC = self.loss_function(self.forward(x_BC), y_BC)\n",
    "        return loss_BC\n",
    "\n",
    "    def lossPDE(self, x_PDE):\n",
    "        _x, _gamma = x_PDE[:, 0].unsqueeze(1), x_PDE[:, 1].unsqueeze(1)\n",
    "        x_PDE.requires_grad = True  # Enable differentiation\n",
    "        f = self.forward(x_PDE)\n",
    "        f_x = torch.autograd.grad(f, x_PDE, torch.ones(x_PDE.shape[0], 1), retain_graph=True, create_graph=True)[0]\n",
    "        f_x = f_x[:, 0].unsqueeze(1)\n",
    "        func = f_x * _x ** 2\n",
    "        f_xx = torch.autograd.grad(func, x_PDE, torch.ones(x_PDE.shape[0], 1), retain_graph=True, create_graph=True)[0]\n",
    "        power1 = 2 - _gamma\n",
    "        power2 = 4 - _gamma\n",
    "        cout = f_xx - (2 * _x ** power1) / (_x + 1) ** power2\n",
    "        return self.loss_function(cout, f_hat)\n",
    "\n",
    "    def loss(self, x_BC, y_BC, x_PDE):\n",
    "        loss_bc = self.lossBC(x_BC, y_BC)\n",
    "        loss_pde = self.lossPDE(x_PDE)\n",
    "        return loss_bc + loss_pde\n",
    "\n",
    "    # Optimizer              X_train_Nu,Y_train_Nu,X_train_Nf\n",
    "    def closure(self):\n",
    "        optimizer.zero_grad()\n",
    "        loss = self.loss(X_train_Nu, Y_train_Nu, X_train_Nf)\n",
    "        loss.backward()\n",
    "        self.iter += 1\n",
    "        if self.iter % 100 == 0:\n",
    "            loss2 = self.lossBC(X_test, Y_test)\n",
    "            print(\"Training Error:\", loss.detach().cpu().numpy(), \"---Testing Error:\", loss2.detach().cpu().numpy())\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e99c10b-9705-4200-a131-e04c9cbc2ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(x_min, x_max, total_points_x).view(-1,1)\n",
    "gamma = torch.linspace(gamma_min, gamma_max, total_points_gamma).view(-1,1)\n",
    "# Create the mesh \n",
    "X,GAMMA = torch.meshgrid(x.squeeze(1), gamma.squeeze(1), indexing='xy')\n",
    "y_real = dehnen(X,GAMMA)\n",
    "# plot3D(x, gamma, y_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1889fd10-efcf-4bdd-a109-d14f6f6c03de",
   "metadata": {},
   "source": [
    "## Initial Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d716bfb-c994-48f3-a4c1-61a2b3250e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape, gamma.shape)\n",
    "print(X.shape, GAMMA.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae63efae-0b0f-4198-8c82-fa843ea9e310",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Left Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddf6dc5-9a1f-45bc-8298-476158ba218d",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_X = torch.hstack((X[:, 0][:,None],GAMMA[:, 0][:,None])) # First column # The [:,None] is to give it the right dimension\n",
    "left_Y = dehnen(left_X[:,0], GAMMA[:,0]).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6925e782-57e9-4d6e-a11e-b2dd50015dec",
   "metadata": {},
   "source": [
    "### Bottom Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c200a475-e1b1-4aa6-94ae-b74c44948fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_X = torch.hstack((X[0,:][:,None],GAMMA[0,:][:,None])) # First row # The [:,None] is to give it the right dimension\n",
    "bottom_Y = dehnen(bottom_X[:, 0], GAMMA[0, :]).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606cf863-2c44-4ca2-bf6a-14ebca6fd87f",
   "metadata": {},
   "source": [
    "### Upper Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d381c59b-3b2b-467c-9887-4184dbfed9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_X = torch.hstack((X[-1,:][:,None],GAMMA[-1, :][:,None])) # Last row # The [:,None] is to give it the right dimension\n",
    "top_Y = dehnen(top_X[:, 0], GAMMA[-1, :]).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d9f004-2bfa-497c-9e44-e8eea182a3ec",
   "metadata": {},
   "source": [
    "### Right Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c9a848-6436-4cd3-b4ae-7f11725f9026",
   "metadata": {},
   "outputs": [],
   "source": [
    "right_X = torch.hstack((X[:, -1][:,None],GAMMA[:, -1][:,None]))\n",
    "right_Y = dehnen(right_X[:, 0], GAMMA[:, 0]).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6d8504-25b2-4714-a6d1-7daa94a39a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the mesh into a 2-column vector\n",
    "x_test = torch.hstack((X.transpose(1,0).flatten()[:,None],GAMMA.transpose(1,0).flatten()[:,None]))\n",
    "y_test = y_real.transpose(1,0).flatten()[:,None] # Colum major Flatten (so we transpose it)\n",
    "# Domain bounds\n",
    "lb = x_test[0]  # first value\n",
    "ub = x_test[-1] # last value \n",
    "print(x_test.shape, y_test.shape)\n",
    "print(lb, ub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c931b26b-6b29-4fcc-9414-81c0e4a679f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.vstack([left_X,bottom_X,top_X, right_X])\n",
    "Y_train = torch.vstack([left_Y,bottom_Y,top_Y, right_Y])\n",
    "#Choose(Nu) points of our available training data:\n",
    "idx = np.random.choice(X_train.shape[0] , Nu, replace=False)\n",
    "X_train_Nu = X_train[idx,:]\n",
    "Y_train_Nu = Y_train[idx,:]\n",
    "# Collocation Points (Evaluate our PDe)\n",
    "#Choose(Nf) points(Latin hypercube)\n",
    "X_train_Nf = lb + (ub - lb) * lhs(2, Nf) # 2 as the inputs are x and t\n",
    "X_train_Nf = torch.vstack((X_train_Nf, X_train_Nu)) # Add the training points to the collocation point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0713aa-f43a-4058-bec0-9a4f2f5b0ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "torch.manual_seed(123)\n",
    "#Store tensors to GPU\n",
    "X_train_Nu = X_train_Nu.float() # Training Points (BC)\n",
    "Y_train_Nu = Y_train_Nu.float() # Training Points (BC)\n",
    "X_train_Nf = X_train_Nf.float() # Collocation Points\n",
    "f_hat = torch.zeros(X_train_Nf.shape[0],1)#to minimize function\n",
    "\n",
    "X_test = x_test.float() # the input dataset (complete)\n",
    "Y_test = y_test.float() # the real solution \n",
    "\n",
    "\n",
    "#Create Model\n",
    "PINN = FCN(layers)\n",
    "\n",
    "print(PINN)\n",
    "\n",
    "optimizer = torch.optim.Adam(PINN.parameters(),lr=lr,amsgrad=False)\n",
    "'''\n",
    "'L-BFGS Optimizer'\n",
    "optimizer = torch.optim.LBFGS(PINN.parameters(), lr=lr, \n",
    "                              max_iter = steps, \n",
    "                              max_eval = None, \n",
    "                              tolerance_grad = 1e-05, \n",
    "                              tolerance_change = 1e-09, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56db212f-f0be-4e98-b691-1ca234f3a0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "for i in range(steps):\n",
    "    if i==0:\n",
    "        print(\"Training Loss-----Test Loss\")\n",
    "    loss = PINN.loss(X_train_Nu,Y_train_Nu,X_train_Nf)# use mean squared error\n",
    "    losses.append(loss)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i%(steps/10)==0:\n",
    "        with torch.no_grad():\n",
    "            test_loss = PINN.lossBC(X_test,Y_test)\n",
    "        print(loss.detach().numpy(),'---',test_loss.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8ff2c6-01d1-4abb-9686-43b18ea43113",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = [t.detach() for t in losses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b45679-b5e4-4889-8f7d-90957fd239d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd4d4d0-92fb-44fc-b772-7bb08582eaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = PINN(X_test)\n",
    "x1 = X_test[:,0]\n",
    "t1 = X_test[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8728e21f-16f0-434c-939c-9c114202dbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_x1=x1.reshape(shape=[100,200]).transpose(1,0).detach().cpu()\n",
    "arr_T1=t1.reshape(shape=[100,200]).transpose(1,0).detach().cpu()\n",
    "arr_y1=y1.reshape(shape=[100,200]).transpose(1,0).detach().cpu()\n",
    "arr_y_test=y_test.reshape(shape=[100,200]).transpose(1,0).detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215a5ff8-9996-4937-9611-76d0fa3e28a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "\n",
    "def plot3D(x,t,y):\n",
    "    x_plot =x.squeeze(1) \n",
    "    t_plot =t.squeeze(1)\n",
    "    X,T= torch.meshgrid(x_plot,t_plot)\n",
    "    F_xt = y\n",
    "    fig,ax=plt.subplots(1,1)\n",
    "    cp = ax.contourf(T,X, F_xt,20,cmap=\"rainbow\")\n",
    "    fig.colorbar(cp) # Add a colorbar to a plot\n",
    "    ax.set_title('F(x,t)')\n",
    "    ax.set_xlabel('t')\n",
    "    ax.set_ylabel('x')\n",
    "    plt.show()\n",
    "    ax = plt.axes(projection='3d')\n",
    "    ax.plot_surface(T.numpy(), X.numpy(), F_xt.numpy(),cmap=\"rainbow\")\n",
    "    ax.set_xlabel('t')\n",
    "    ax.set_ylabel('x')\n",
    "    ax.set_zlabel('f(x,t)')\n",
    "    plt.show()\n",
    "\n",
    "def plot3D_Matrix(x,t,y):\n",
    "    X,T= x,t\n",
    "    F_xt = y\n",
    "    fig,ax=plt.subplots(1,1)\n",
    "    cp = ax.contourf(T,X, F_xt,20,cmap=\"rainbow\")\n",
    "    fig.colorbar(cp) # Add a colorbar to a plot\n",
    "    ax.set_title('F(x,t)')\n",
    "    ax.set_xlabel('t')\n",
    "    ax.set_ylabel('x')\n",
    "    plt.show()\n",
    "    ax = plt.axes(projection='3d')\n",
    "    ax.plot_surface(T.numpy(), X.numpy(), F_xt.numpy(),cmap=\"rainbow\")\n",
    "    ax.set_xlabel('t')\n",
    "    ax.set_ylabel('x')\n",
    "    ax.set_zlabel('f(x,t)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6886f52-b227-43c7-96d6-aa726d9381e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot3D_Matrix(arr_x1,arr_T1,arr_y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf2ef49-f06a-408d-acc5-c93a3a4a7c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1\n",
    "num_points = 1000\n",
    "domain = np.linspace(x_min, x_max, num_points)\n",
    "x_test = np.array([domain, np.ones(num_points) * gamma])\n",
    "\n",
    "y_pred = PINN(x_test.T)\n",
    "\n",
    "plt.plot(domain, y_pred.detach().numpy(), '--r', label=\"Prediction\")\n",
    "plt.plot(domain, dehnen(domain, gamma), label=\"Analytical Value\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667415be-0beb-4743-a75c-46dd70fcda6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
