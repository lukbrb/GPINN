{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "662d9ddb-f3fa-4c82-956f-0689c89fbc08",
   "metadata": {},
   "source": [
    "# PINN Solver for Hernquist profile\n",
    "\n",
    "## 1. Introduction\n",
    "We start from the Hernquist density profile $$\\rho(r) = \\dfrac{M}{2\\pi}\\dfrac{a}{r}\\dfrac{1}{(r+a)} $$\n",
    "\n",
    "For such a potential, the Poisson equation reads:\n",
    "$$\\nabla ^2 \\phi = 4\\pi G \\rho$$\n",
    "\n",
    "$$ \\Leftrightarrow \\dfrac{1}{r^2} \\dfrac{\\partial}{\\partial r}\\left(r^2 \\dfrac{\\partial \\phi}{\\partial r}\\right) = 4\\pi G \\left[\\dfrac{M}{2\\pi}\\dfrac{a}{r}\\dfrac{1}{(r+a)}\\right]$$\n",
    "\n",
    "Given that the density-potential pair depends solely on the radial coordinate $r$, the equation reduces to\n",
    "$$  \\dfrac{1}{r} \\dfrac{d}{d r}\\left(r^2 \\dfrac{d \\phi}{d r}\\right) = \\dfrac{2GMa}{a^3(\\frac{r}{a}+1)^3}$$\n",
    "\n",
    "\n",
    "Setting $r \\rightarrow \\frac{r}{a}$, we get\n",
    "$$  \\dfrac{d}{d r}\\left(r^2 \\dfrac{d \\phi}{d r}\\right) = \\dfrac{2GM}{a}\\dfrac{r}{(r+1)^3}$$\n",
    "\n",
    "\n",
    "Finally, in the case of the Hernquist profile, the Poisson equation can be written such as:\n",
    "\n",
    "$$ \\boxed{\\dfrac{d}{d r}\\left(r^2 \\dfrac{d \\phi}{d r}\\right) = \\dfrac{2r}{(r+1)^3}}$$\n",
    "\n",
    "where we have set $\\frac{GM}{a}$ to unity.\n",
    "\n",
    "By [integrating Poisson's equation](/analytical_potential_hernquist.ipynb), we find for this non dimensional equation :\n",
    "\n",
    "$$ \\Phi(s) = -\\dfrac{1}{s + 1} $$ where $s=\\dfrac{r}{a}$.\n",
    "\n",
    "## 2. Implementation of the Physics-Informed Neural Network\n",
    "\n",
    "We implement a simple neural network with one hidden layer (to start with), which outputs a single value. The latter value corresponds to the amplitude of the field at the point of interest. \n",
    "\n",
    "### 2.1 The Neural Network\n",
    "\n",
    "We use here a Feedforward Neural Network (FFNN). The input layer takes one co-location point as an input. It has one hidden layer composed of 100 neurons, and outputs the amplitude of the field at this point. The neural network is here used as a universel function approximator. We use for the activation function the hyperbolic tangent. \n",
    "\n",
    "Several architectures will be tried out for the FFNN: \n",
    "\n",
    "|Number of layers| Number of Neurons (per layer) | Activation function  | Article  | \n",
    "|----------------|-------------------------------|----------------------|----------|\n",
    "|   2-4 layers   |   32 and 50 neurons           |                      |          |\n",
    "|   5-8 layers   |   250 neurons                 |                      |          |\n",
    "|   9+  layers   |   20 neurons                  |                      |          |\n",
    "\n",
    "### 2.2 The Loss Function\n",
    "\n",
    "As stated, the model is trained by minimizing the loss function. In the case of Physics-Informed Neural Networks (PINNs), the loss function can be written in a general way as :\n",
    "\n",
    "$$ \\mathcal{L}(\\theta) = \\omega_{\\mathcal{F}}\\mathcal{L}_{\\mathcal{F}}(\\theta) + \\omega_{\\mathcal{B}}\\mathcal{L}_{\\mathcal{B}}(\\theta) + \\omega_{\\mathcal{d}}\\mathcal{L}_{data}(\\theta)$$ \n",
    "where $\\theta$ are the parameters of the neural network, and \n",
    "$$\\mathcal{L}_{\\mathcal{F}}(\\theta) = MSE_{\\mathcal{F}} = \\dfrac{1}{N_c}\\sum^{N_c}_{i=1} \\left\\|\\mathcal{F}(\\hat{u}_{\\theta}(z_i)) - f(z_i)\\right\\|^2$$ represents the loss produced by a mismatch with the governing differential equation $\\mathcal{F}$.\n",
    "\n",
    "Similarly we have the following expression for the boundary loss :\n",
    "\n",
    "$$\\mathcal{L}_{\\mathcal{B}}(\\theta) = MSE_{\\mathcal{B}} = \\dfrac{1}{N_B}\\sum^{N_B}_{i=1} \\left\\|\\mathcal{B}(\\hat{u}_{\\theta}(z_i)) - g(z_i)\\right\\|^2$$\n",
    "\n",
    "$N_c$ and $N_B$ represent the number of collocation and boundary points, respectively. \n",
    "\n",
    "$\\mathcal{L}_{data}(\\theta)$ is the loss with respect to the data points:\n",
    "\n",
    "$$\\mathcal{L}_{data}(\\theta) = MSE_{data} = \\dfrac{1}{N_d}\\sum^{N_d}_{i=1} \\left\\|\\hat{u}_{\\theta}(z_i)) - u^{*}_i\\right\\|^2$$\n",
    "\n",
    "Note that the quantity $r_{\\mathcal{F}}[\\hat{u}_{\\theta}](z) = r_{\\theta} (z) := \\mathcal{F}(\\hat{u}_{\\theta}(z_i); \\gamma) - f(z_i)$ is the differential equation residual. Similarly, the residual NN corresponding to boundary conditions is $r_{\\mathcal{B}}[\\hat{u}_{\\theta}](z) = r_{\\theta} (z) := \\mathcal{B}(\\hat{u}_{\\theta}(z)) - g(z)$ [[1](https://doi.org/10.1007/s10915-022-01939-z)].\n",
    "\n",
    "**Loss function for the Poisson equation**\n",
    "\n",
    "In our case, we do not consider any data points. Therefore $N_d$ = 0 and $\\mathcal{L}_{data}(\\theta)=0$. Also we consider for now that $\\omega_{\\mathcal{F}} = \\omega_{\\mathcal{B}} = 1$. Finally, we consider in the training a 1-D domain, with $N_c = 1000$ and $N_B = 1$. <font color='red'>Note: Should we consider external boundary condition, and not solely the initial one ?</font>\n",
    "\n",
    "We note that here, the residual $r_{\\theta} (z)$ is as follow :\n",
    "\n",
    "$$ r_{\\theta} (z) = \\dfrac{d}{d r}\\left(r^2 \\dfrac{d \\phi}{d r}\\right) - \\dfrac{2r}{(r+1)^3}$$\n",
    "\n",
    "Therefore, we want to minimize the following loss function :\n",
    "\n",
    "$$ \\mathcal{L}(\\theta) = \\dfrac{1}{N_c}\\sum^{N_c}_{i=1} \\left\\|\\dfrac{df}{d r} - \\dfrac{2r_i}{(r_i+1)^3} \\right\\|^2 + \\dfrac{1}{N_B}\\sum^{N_B}_{i=1} \\left\\|\\hat{\\Phi}(z_0) - \\Phi_0 \\right\\|^2$$ \n",
    "\n",
    "<font color='red'>Unclear to me what the $\\mathcal{B}$ operator and the boundary function $g$ are.</font>\n",
    "where we set $f = r^2 \\dfrac{d \\phi}{d r}$. \n",
    "\n",
    "**Boundary conditions:** We create a domain for which $r \\in [a; b]$.\n",
    "Hernquist shows that by integrating the aforementionned Poisson equation, one gets that:\n",
    "\n",
    "$$ \\Phi(r) = - \\dfrac{GM}{r + a}$$\n",
    "\n",
    "Let us recall that we have set $\\frac{GM}{a} = 1$, meaning that $GM=a$. Therefore, in our case we should rewrite the potential such as :\n",
    "\n",
    "$$ \\Phi(r) = - \\dfrac{1}{r + a}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d25e63d-c0f0-43e2-9f53-4069c08e3875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from pyDOE import lhs\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8-darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75e0536-e673-4a02-84e6-82d809b2ee18",
   "metadata": {},
   "source": [
    "**Implementation of the Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5d54f6-37c2-46bd-8d41-97148ba603b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, num_hidden: int, dim_hidden: int, act=nn.Tanh()):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_in = nn.Linear(1, dim_hidden)\n",
    "        self.layer_out = nn.Linear(dim_hidden, 1)\n",
    "\n",
    "        num_middle = num_hidden - 1\n",
    "        self.middle_layers = nn.ModuleList([nn.Linear(dim_hidden, dim_hidden) for _ in range(num_middle)])\n",
    "        self.act = act\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.act(self.layer_in(x))\n",
    "        for layer in self.middle_layers:\n",
    "            out = self.act(layer(out))\n",
    "        return self.layer_out(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb22aa0-1575-4190-bc53-f37239a0e78a",
   "metadata": {},
   "source": [
    "**Implementation of the derivatives for the loss function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5360d777-dc85-4040-bc0e-c1d79e459d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pde_residual(net, x):\n",
    "    phi_pred = net(x)\n",
    "    phi_x = torch.autograd.grad(phi_pred, x, grad_outputs=torch.ones_like(x), create_graph=True, retain_graph=True,)[0]\n",
    "    phi_x *= x ** 2\n",
    "    df = torch.autograd.grad(phi_x, x, grad_outputs=torch.ones_like(x), create_graph=True, retain_graph=True,)[0]\n",
    "    residual = df - 2 * x / (x + 1) ** 3\n",
    "    return residual.pow(2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bef34ae-9efa-4f6e-8e60-6981231beab5",
   "metadata": {},
   "source": [
    "**Implementation of the loss function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fec052-302c-466b-90c4-591846f5fafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundary_error(net, x_b, y_true):\n",
    "    phi_pred = net(x_b)\n",
    "    error = y_true - phi_pred\n",
    "    return error.pow(2).mean()\n",
    "\n",
    "def loss_function(net, x, x_b, y_true):\n",
    "    return pde_residual(net, x) + boundary_error(net, x_b, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac4bedb-8e9d-4768-8937-436380033c3c",
   "metadata": {},
   "source": [
    "**Implementation of the training phase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa68fe8-aafd-4b20-8797-4fcf76096361",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingPhase:\n",
    "    def __init__(self, neural_net, domain, n_epochs, optimizer, _loss_function, x_train, y_train):\n",
    "        self.neural_net = neural_net\n",
    "        self.domain = domain\n",
    "        self.n_epochs = n_epochs\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_function = _loss_function\n",
    "        self.x_train = x_train\n",
    "        self.y_true = y_train\n",
    "\n",
    "    def train_model_general(self, lr, x_val, y_val):\n",
    "        opt = self.optimizer(self.neural_net.parameters(), lr=lr)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,\n",
    "                                                           mode='min',\n",
    "                                                           factor=0.5,\n",
    "                                                           patience=1000,\n",
    "                                                           threshold=0.0001,\n",
    "                                                           verbose=False)\n",
    "        loss = self.loss_function\n",
    "\n",
    "        losses = []\n",
    "        val_losses = []\n",
    "        epochs = []\n",
    "        for epoch in trange(self.n_epochs, desc=\"Epoch\"):\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss_value = loss(self.neural_net, self.domain, self.x_train, self.y_true)\n",
    "            loss_value.backward()\n",
    "            opt.step()\n",
    "            scheduler.step(loss_value)\n",
    "            \n",
    "            val_loss = (y_val - self.neural_net(x_val)).pow(2).mean()\n",
    "            losses.append(loss_value.item())\n",
    "            val_losses.append(val_loss.item())\n",
    "        return self.neural_net, np.array(epochs), np.array(losses), np.array(val_losses)\n",
    "    \n",
    "    def train_model_LBFGS(self, lr):\n",
    "    # Remplacez votre optimisateur par L-BFGS\n",
    "        opt = torch.optim.LBFGS(self.neural_net.parameters(), lr=lr)\n",
    "        loss = self.loss_function\n",
    "\n",
    "        losses = []\n",
    "        val_losses = []\n",
    "        epochs = []\n",
    "\n",
    "        # Créer une fonction de fermeture pour calculer la perte.\n",
    "        def closure():\n",
    "            opt.zero_grad()\n",
    "            loss_value = loss(self.neural_net, self.domain, self.x_train, self.y_true)\n",
    "            loss_value.backward()\n",
    "            return loss_value\n",
    "\n",
    "        for epoch in trange(self.n_epochs, desc=\"Epoch\"):\n",
    "            # L'optimisation L-BFGS requiert l'appel à la méthode step avec la fermeture de la fonction de perte\n",
    "            opt.step(closure)\n",
    "            current_loss = closure()\n",
    "            val_loss = (y_val - self.neural_net(x_val)).pow(2).mean()\n",
    "            losses.append(current_loss.item())\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "        return self.neural_net, np.array(epochs), np.array(losses), np.array(val_losses)\n",
    "    \n",
    "    def train_model(self, lr, x_val, y_val):\n",
    "        if self.optimizer == 'LBFGS':\n",
    "            print(\"Training with LBFGS optimizer\")\n",
    "            net, epochs, losses, val_losses = self.train_model_LBFGS(lr, x_val, y_val)\n",
    "        else:\n",
    "            net, epochs, losses, val_losses = self.train_model_general(lr, x_val, y_val)\n",
    "        return net, epochs, losses, val_losses\n",
    "    \n",
    "    def save_model(self, filename):\n",
    "        torch.save(self.neural_net, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c33a79-bcff-486c-8f7e-dc6effeb6317",
   "metadata": {},
   "source": [
    "**Training the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ff9688-f2c5-4aad-993f-be2cee7adcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analytic_prediction(r):\n",
    "    \"\"\" Value of the gravitational potential \n",
    "        in the case of an Hernquist profile \n",
    "    \"\"\"\n",
    "    return -1/(r + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c5fd1f-e560-47b8-b01d-1cbd5f509d31",
   "metadata": {},
   "source": [
    "**Generating Training Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2631f3-a2da-49e3-aa99-372922aa3e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial conditions\n",
    "Npoints = 10_000\n",
    "Nf = 1024  # collocation points\n",
    "Nvalidation = Nf // 3\n",
    "x0 = 0.0\n",
    "f0 = analytic_prediction(x0)\n",
    "xf = 1000\n",
    "ff = analytic_prediction(xf)\n",
    "# Domain\n",
    "torch.manual_seed(0)\n",
    "s = torch.linspace(x0, xf, Npoints)\n",
    "s_val = (x0 + (xf - x0) * torch.rand(Nvalidation)).reshape(-1, 1)\n",
    "phi_val = analytic_prediction(s_val)\n",
    "\n",
    "S_train_Nf = x0 + (xf - x0) * lhs(1, Nf)\n",
    "\n",
    "x_train = torch.Tensor([x0, xf]).reshape(-1, 1)\n",
    "y_train = torch.Tensor([f0, ff]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f13f216-243f-4bda-a167-c50edb86a3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.title(\"Configuration of training points\")\n",
    "plt.plot(S_train_Nf, np.zeros_like(S_train_Nf), 'b.', label='Collocation Points')\n",
    "plt.plot(x_train, [0, 0], 'rx', label='Boundary Points')\n",
    "# Ajouter du texte avec les coordonnées (x_train, y_train)\n",
    "for i in range(len(x_train)):\n",
    "    plt.text(x_train[i]-100, 0.05, f'({float(x_train[i]): .3f}, {float(y_train[i]):.3f})')\n",
    "\n",
    "plt.xlabel('$s$')\n",
    "plt.xlim((-100, 1150))\n",
    "plt.ylim((-0.5, 0.5))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47cfd86-67c1-4dd2-84be-fe24aaf0f97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276cb3c5-98d0-49d2-a914-d4b3342b4129",
   "metadata": {},
   "outputs": [],
   "source": [
    "S_train_Nf = torch.Tensor(S_train_Nf)\n",
    "S_train_Nf.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0a0348-f395-4398-bff7-3599f08c3459",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8e0a32-bb65-4d04-9333-4f3896be417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "n_epochs = 100_000\n",
    "a = 1.0\n",
    "NUM_HIDDEN_LAYERS = 2\n",
    "DIM_HIDDEN_LAYERS = 32\n",
    " \n",
    "pinn = NeuralNet(num_hidden=NUM_HIDDEN_LAYERS, dim_hidden=DIM_HIDDEN_LAYERS, act=nn.Tanh())\n",
    "print(\"Training Neural Network :\", pinn)\n",
    "\n",
    "\n",
    "\n",
    "training = TrainingPhase(neural_net=pinn, domain=S_train_Nf, n_epochs=n_epochs, \n",
    "                         optimizer=torch.optim.Adam,_loss_function=loss_function, x_train=x_train, y_train=y_train)\n",
    "\n",
    "net, epochs, losses, val_losses = training.train_model(lr=1e-5, x_val=s_val, y_val=phi_val)\n",
    "# training.save_model(f\"resultats/Hernquist_{n_epochs}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3ede12-500d-4e26-ae86-e873c8e67df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_domain = torch.linspace(0, 100, 15000, requires_grad=False) * a\n",
    "y_true = analytic_prediction(testing_domain.numpy())\n",
    "y_pred_plot = net(testing_domain.reshape(-1, 1)).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993df168-1df3-4d74-ad70-dd868fdc73e1",
   "metadata": {},
   "source": [
    "**Plotting the results**  \n",
    "This part can be run independently from the rest of the notebook, if the model has already been trained once. Indeed the results are saved so the model does not have to be trained each time. To do so, go below to the \"[Plotting using pre-trained model\" section](#Plotting-using-pre-trained-model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c3484c-8809-42cc-9088-ca98c63b1fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(f\"Hernquist model\")\n",
    "plt.plot(testing_domain.numpy(), y_true, 'b', label=\"Analytical Value\")\n",
    "plt.plot(testing_domain.numpy(), y_pred_plot.detach().numpy(), '--r', label=\"Predicted Value\")\n",
    "plt.xlabel('$s$')\n",
    "plt.ylabel('$\\Phi(s)$')\n",
    "# plt.plot(s.detach().numpy(), y_pred_train.detach().numpy(),'.k', label=\"Predicted 2\")\n",
    "# plt.savefig(f\"resultats/Predicttion_{NUM_HIDDEN_LAYERS}x{DIM_HIDDEN_LAYERS}_linear.png\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bb7b3b-b7ee-4785-beb2-9ec0920d6ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac318890-9af0-40dc-b902-e0936c57f5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff133b3e-5628-4718-bdad-8f7ea1652ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "(32 + 32) + (32 * 32 + 32) + (32 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcf6bbf-a29f-497d-8630-3bdf64749d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_error(domain, y_pred, y_true, plot=False, save=False):\n",
    "    result = (y_pred - y_true) / y_true\n",
    "    print(np.mean(result))\n",
    "    if not plot:\n",
    "        return result\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(r\"Relative error \")\n",
    "    plt.plot(domain, result)\n",
    "    plt.xlabel('$s$')\n",
    "    plt.ylabel('$\\dfrac{\\Delta \\phi}{\\phi}$')\n",
    "    if save:\n",
    "            plt.savefig(f\"Relative_error_hernquist_{a=}.png\")\n",
    "    plt.show()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b946a0-41bc-452f-a395-5c96420b30da",
   "metadata": {},
   "outputs": [],
   "source": [
    "relat_err = relative_error(domain=testing_domain, y_pred=y_pred_plot.detach().numpy(), y_true=y_true, plot=True, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7814d05-488b-4944-b436-83ac01429e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Loss Function\")\n",
    "plt.plot(np.arange(n_epochs), losses, 'k', label=\"Training Loss\")\n",
    "plt.plot(np.arange(n_epochs), val_losses, 'r', label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss value\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend();\n",
    "#plt.savefig(f\"resultats/Loss_{NUM_HIDDEN_LAYERS}x{DIM_HIDDEN_LAYERS}_log.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c767fd3a-0f36-4d48-9fea-46939f5982de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "# Définition de la fonction qui représente le système d'équations différentielles\n",
    "def system(s, y):\n",
    "    return [y[1], (2*s/(s+1)**3) / s**2]\n",
    "\n",
    "# Conditions initiales\n",
    "s0 = 0.01 # Evite division par 0, à ajuster selon votre problème\n",
    "Phi_prime0 = 0 # Phi'(0)\n",
    "Phi_double_prime0 = -1 # Phi''(0)\n",
    "\n",
    "y0 = [Phi_prime0, Phi_double_prime0]\n",
    "\n",
    "# Grille pour la solution\n",
    "s = np.linspace(s0, 10, 1000)  # Ajustez selon vos besoins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a514692a-6e77-4b76-bd1c-a961b2d10af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Résolution du système d'équations différentielles\n",
    "sol = solve_ivp(system, [s0, 10], y0, t_eval=s, method='RK45')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3f4af9-2af1-4c76-b423-2a3664e25f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(s, sol.y[1], label=\"Computed value\")\n",
    "plt.plot(s, analytic_prediction(s), '--r', label=\"Analytical Value\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333f7029-b6e2-4e74-8cc8-f8f233cab8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = testing_domain.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c4f0c9-bc26-453d-a492-47c2f6b4a4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "net(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fc7e67-8803-413a-8c92-ed1db7457431",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
